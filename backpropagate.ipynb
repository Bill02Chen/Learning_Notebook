{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5M8ECGxKIjF"
      },
      "source": [
        "First, let us import all necessary libraries for this one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us9G7e71-hbI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import math\n",
        "import time\n",
        "from typing import List, Union, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHXpR5s0M_gD"
      },
      "source": [
        "Problem A. Back propagation\n",
        "=="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym1FBDpuVdMf"
      },
      "source": [
        "In this problem, let us try to implement the back propagation by ourselves.\n",
        "\n",
        "Requirement:\n",
        "* You cannot reuse the any pytorch backward function\n",
        "* You cannot use any auto-grad library\n",
        "* You can use pytorch forward function, or numpy utility functions, unless we explicitly mention that certain functions cannot be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YojCqobqWEdA"
      },
      "source": [
        "## Utility functions for verifying the gradient results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqNbf1i2W9_F"
      },
      "source": [
        "First, let us create a base class, which you will need to create your inheritted class later. Do not change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "001Kr5SGWNwm"
      },
      "outputs": [],
      "source": [
        "#@title Base class for all your own implementation.\n",
        "\n",
        "class CustomizedLayer(object):\n",
        "\n",
        "  def set_params(self, param_dict: Dict[str, torch.Tensor]) -> None:\n",
        "    \"\"\"\n",
        "    Set the parameters of the layer, like weight and bias of linear layer.\n",
        "\n",
        "    Args:\n",
        "      param_dict: A dictionary of parameters.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def forward(self, inputs: List[np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Forward pass of the layer.\n",
        "\n",
        "    Args:\n",
        "      inputs: A list of input tensors.\n",
        "\n",
        "    Returns:\n",
        "      The output of the layer.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def input_gradients(\n",
        "      self,\n",
        "      inputs: List[np.ndarray],\n",
        "      outputs: np.ndarray,\n",
        "      output_gradient: np.ndarray\n",
        "  ) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Calculate the input gradients of the layer.\n",
        "    Args:\n",
        "      inputs: A list of input tensors.\n",
        "      outputs: The output of the layer.\n",
        "      output_gradient: The gradient of the output.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def param_gradients(\n",
        "      self,\n",
        "      inputs: List[np.ndarray],\n",
        "      outputs: np.ndarray,\n",
        "      output_gradient: np.ndarray\n",
        "  ) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Calculate the parameter gradients of the layer.\n",
        "    Args:\n",
        "      inputs: A list of input tensors.\n",
        "      outputs: The output of the layer.\n",
        "      output_gradient: The gradient of the output.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykfug5EfWBae"
      },
      "source": [
        "Then, define a few utility functions for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdNMmiAAXNnM"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "\n",
        "def verify_output(\n",
        "    actual: Union[np.ndarray, List[np.ndarray]],\n",
        "    reference: Union[np.ndarray, List[np.ndarray]],\n",
        "    name: Union[str, List[str]],\n",
        "    atol: float = 1e-6,\n",
        "    rtol: float = 1e-6\n",
        ") -> None:\n",
        "  \"\"\"\n",
        "  Verify if the actual output is close to the reference output.\n",
        "\n",
        "  Args:\n",
        "    actual: The actual output.\n",
        "    reference: The reference output.\n",
        "    atol: The absolute tolerance.\n",
        "    rtol: The relative tolerance.\n",
        "  \"\"\"\n",
        "  if isinstance(actual, np.ndarray):\n",
        "    close = np.allclose(actual, reference, atol=atol, rtol=rtol)\n",
        "    if not close:\n",
        "      print(f\"{name} is not close to the reference output.\")\n",
        "    else:\n",
        "      print(f\"{name} is close to the reference output.\")\n",
        "  elif isinstance(actual, List):\n",
        "    close = all(verify_output(a, b, name[i], atol=atol, rtol=rtol)\n",
        "                for i, (a, b) in enumerate(zip(actual, reference)))\n",
        "  return close\n",
        "\n",
        "def compare_with_actual_layer(\n",
        "    official_layer: nn.modules,\n",
        "    customized_layer: CustomizedLayer,\n",
        "    input_np: List[np.ndarray],\n",
        "    output_grad_np: np.ndarray,\n",
        "    parameter_names: List[str],\n",
        "    param_np: List[np.ndarray],\n",
        "    a_tol: float = 1e-6,\n",
        "    r_tol: float = 1e-6,\n",
        ") -> bool:\n",
        "  \"\"\"\n",
        "  Compare forward/backward of your customized layer with the official layer.\n",
        "\n",
        "  Args:\n",
        "    official_layer: The official layer.\n",
        "    customized_layer: The customized layer.\n",
        "    input_np: The input numpy array.\n",
        "    output_grad_np: The output gradient numpy array.\n",
        "    parameter_names: The names of the parameters.\n",
        "    param_np: The numpy array of the parameters.\n",
        "    a_tol: The absolute tolerance.\n",
        "    r_tol: The relative tolerance.\n",
        "\n",
        "  Returns:\n",
        "    True if the output of the customized layer is close to the output of\n",
        "    the official layer, False otherwise.\n",
        "  \"\"\"\n",
        "  success = True\n",
        "  input_torch = [\n",
        "      torch.tensor(x, requires_grad=True) for x in input_np\n",
        "  ]\n",
        "  output_grad_torch = torch.tensor(output_grad_np, requires_grad=True)\n",
        "  for i, name in enumerate(parameter_names):\n",
        "    setattr(official_layer, name,\n",
        "            nn.Parameter(torch.tensor(param_np[i], requires_grad=True)))\n",
        "\n",
        "  # Run forward and backward on official layers.\n",
        "  output_official_torch = official_layer(*input_torch)\n",
        "  output_official_torch.backward(output_grad_torch)\n",
        "  output_official_np = output_official_torch.detach().numpy()\n",
        "\n",
        "  # Check output\n",
        "  output_customized_np = customized_layer.forward(input_np)\n",
        "  success = verify_output(output_customized_np, output_official_np,\n",
        "                          \"Output\", atol=a_tol, rtol=r_tol)\n",
        "  if not success:\n",
        "    return success\n",
        "\n",
        "  # Check input gradient\n",
        "  input_grad_customized = customized_layer.input_gradients(\n",
        "      input_np, output_official_np, output_grad_np)\n",
        "  input_grad_official = [x.grad.numpy() for x in input_torch]\n",
        "  success = verify_output(\n",
        "      input_grad_customized,\n",
        "      input_grad_official,\n",
        "      [f'Input Gradient {i}' for i in range(len(input_grad_customized))]\n",
        "  )\n",
        "  if not success:\n",
        "    return success\n",
        "\n",
        "  # Check parameter gradient\n",
        "  parameter_grad_customized = customized_layer.param_gradients(\n",
        "      input_np, output_official_np, output_grad_np)\n",
        "  for name in parameter_names:\n",
        "    success = verify_output(parameter_grad_customized[name],\n",
        "                            getattr(official_layer, name).grad.numpy(),\n",
        "                            f\"Parameter Gradient {name}\")\n",
        "    if not success:\n",
        "      return success\n",
        "\n",
        "  print('All tests passed')\n",
        "  return success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXeIVd7HX4rk"
      },
      "source": [
        "## Question A.1 Back-propagation of ADD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhLu_tCbYMcn"
      },
      "source": [
        "Complete the implementation of the following class below, then run the test below.\n",
        "\n",
        "* We have provided the implementation of forward pass\n",
        "* Complete the backward one `input_gradients`.\n",
        "* Note that Add did not have parameters, so `param_gradients` just return an empty result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VSvqbmpNDda"
      },
      "outputs": [],
      "source": [
        "#@title CustomizedAdd\n",
        "\n",
        "class CustomizedAdd(CustomizedLayer):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def set_params(self, param_dict: Dict[str, torch.Tensor]) -> None:\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs: List[np.ndarray]) -> np.ndarray:\n",
        "    x, y = inputs\n",
        "    return x + y\n",
        "\n",
        "  def input_gradients(\n",
        "    self,\n",
        "    inputs: List[np.ndarray],\n",
        "    outputs: np.ndarray,\n",
        "    output_gradient: np.ndarray\n",
        "  ) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    return [output_gradient, output_gradient]\n",
        "\n",
        "  def param_gradients(\n",
        "      self,\n",
        "      inputs: List[np.ndarray],\n",
        "      outputs: np.ndarray,\n",
        "      output_gradient: np.ndarray\n",
        "  ) -> Dict[str, np.ndarray]:\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciTq2cxCYZ2b"
      },
      "source": [
        "Run the test below. Do not change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRY7uDShYUN3",
        "outputId": "1f55c613-014b-4ca0-deb8-424c6afdbab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing with width=3 and height=3 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Input Gradient 1 is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with width=10 and height=1 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Input Gradient 1 is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with width=100 and height=300 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Input Gradient 1 is close to the reference output.\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "#@title Run test\n",
        "\n",
        "def verify_add():\n",
        "  for width, height in [(3, 3), (10, 1), (100, 300)]:\n",
        "    print(f'=== Testing with width={width} and height={height} ===')\n",
        "    if width == 1:\n",
        "      x1 = np.random.randn(height)\n",
        "      x2 = np.random.randn(height)\n",
        "    else:\n",
        "      x1 = np.random.randn(height, width)\n",
        "      x2 = np.random.randn(height, width)\n",
        "    output_gradient = np.random.randn(height, width)\n",
        "    customized_add = CustomizedAdd()\n",
        "    customized_add.set_params({})\n",
        "    class TorchAdd(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(TorchAdd, self).__init__()\n",
        "      def forward(self, x1, x2):\n",
        "        return x1 + x2\n",
        "    torch_add = TorchAdd()\n",
        "\n",
        "    compare_with_actual_layer(\n",
        "        torch_add,\n",
        "        customized_add,\n",
        "        [x1, x2],\n",
        "        output_gradient,\n",
        "        param_np=[],\n",
        "        parameter_names=[],\n",
        "    )\n",
        "\n",
        "verify_add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PvATYOXZQQX"
      },
      "source": [
        "## Question A.2 Back-propagation of ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt0ibdxrZQQY"
      },
      "source": [
        "Similarly, complete the implementation of the following class below, then run the test below.\n",
        "\n",
        "* We have provided the implementation of forward pass\n",
        "* Complete the backward one `input_gradients`.\n",
        "* Note that Add did not have parameters, so `param_gradients` just return an empty result.\n",
        "\n",
        "Hint: for x=0, you can assume gradient of relu is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhDF3B2JZQQY"
      },
      "outputs": [],
      "source": [
        "#@title CustomizedReLU\n",
        "\n",
        "class CustomizedReLU(CustomizedLayer):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def set_params(self, param_dict: Dict[str, torch.Tensor]) -> None:\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs: List[np.ndarray]) -> np.ndarray:\n",
        "    x = inputs[0]\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "  def input_gradients(\n",
        "    self,\n",
        "    inputs: List[np.ndarray],\n",
        "    outputs: np.ndarray,\n",
        "    output_gradient: np.ndarray\n",
        "  ) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    x = inputs[0]\n",
        "    relu_grad = (x > 0).astype(np.float32)\n",
        "    input_gradient = output_gradient * relu_grad\n",
        "    return [input_gradient]\n",
        "\n",
        "  def param_gradients(\n",
        "      self,\n",
        "      inputs: List[np.ndarray],\n",
        "      outputs: np.ndarray,\n",
        "      output_gradient: np.ndarray\n",
        "  ) -> Dict[str, torch.Tensor]:\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkSsEsPTZQQY"
      },
      "source": [
        "Run the test below. Do not change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHzmRy0MZQQY",
        "outputId": "5e9fa335-3105-473f-c4a3-a3b6fca67ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing with width=3 and height=3 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with width=10 and height=1 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with width=100 and height=300 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "#@title Run test\n",
        "\n",
        "def verify_relu():\n",
        "  for width, height in [(3, 3), (10, 1), (100, 300)]:\n",
        "    print(f'=== Testing with width={width} and height={height} ===')\n",
        "    if width == 1:\n",
        "      x = np.random.randn(height)\n",
        "    else:\n",
        "      x = np.random.randn(height, width)\n",
        "    output_gradient = np.random.randn(height, width)\n",
        "    customized_relu = CustomizedReLU()\n",
        "    customized_relu.set_params({})\n",
        "    torch_relu = nn.ReLU()\n",
        "\n",
        "    compare_with_actual_layer(\n",
        "        torch_relu,\n",
        "        customized_relu,\n",
        "        [x],\n",
        "        output_gradient,\n",
        "        param_np=[],\n",
        "        parameter_names=[],\n",
        "    )\n",
        "\n",
        "verify_relu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQeWZhq1aK9v"
      },
      "source": [
        "## Question A.3 Back-propagation of Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjvSaomgaK9v"
      },
      "source": [
        "Similarly, complete the implementation of the following class below, then run the test below.\n",
        "\n",
        "* This time, we also ask you to implement the forward pass.\n",
        "* And complete the backward pass `input_gradients` and `param_gradients`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_vy1eCfaK9v"
      },
      "outputs": [],
      "source": [
        "#@title CustomizedLinear\n",
        "\n",
        "class CustomizedLinear(CustomizedLayer):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = np.zeros((in_features, out_features))\n",
        "    self.bias = np.zeros(out_features)\n",
        "\n",
        "  def set_params(self, param_dict: Dict[str, torch.Tensor]):\n",
        "    self.weight = param_dict['weight']\n",
        "    self.bias = param_dict['bias']\n",
        "\n",
        "  def forward(self, inputs: List[np.ndarray]) -> np.ndarray:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    x = inputs[0]       #batch size\n",
        "    return x @ self.weight.T + self.bias\n",
        "\n",
        "  def input_gradients(\n",
        "    self,\n",
        "    inputs: List[np.ndarray],\n",
        "    outputs: np.ndarray,\n",
        "    output_gradient: np.ndarray\n",
        "  ) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    input_gradient = output_gradient @ self.weight\n",
        "    return input_gradient\n",
        "\n",
        "  def param_gradients(\n",
        "      self,\n",
        "      inputs: List[np.ndarray],\n",
        "      outputs: np.ndarray,\n",
        "      output_gradient: np.ndarray\n",
        "  ) -> Dict[str, np.ndarray]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    x = inputs[0]\n",
        "    weight_gradient = output_gradient.T @ x\n",
        "    bias_gradient = np.sum(output_gradient, axis=0)\n",
        "    return {'weight': weight_gradient,\n",
        "            'bias': bias_gradient}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSce9HjTaK9v"
      },
      "source": [
        "Run the test below. Do not change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F10lTIslaK9v",
        "outputId": "f6e88efc-6acd-41f8-f19d-7ba6901db2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing with in_features=4 and out_features=8 ===\n",
            "Output is close to the reference output.\n",
            "['Input Gradient 0', 'Input Gradient 1', 'Input Gradient 2'] is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_features=10 and out_features=20 ===\n",
            "Output is close to the reference output.\n",
            "['Input Gradient 0', 'Input Gradient 1', 'Input Gradient 2', 'Input Gradient 3', 'Input Gradient 4'] is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_features=100 and out_features=300 ===\n",
            "Output is close to the reference output.\n",
            "['Input Gradient 0', 'Input Gradient 1'] is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "#@title Run test\n",
        "\n",
        "def verify_linear():\n",
        "  for batch_size, in_features, out_features in [(3, 4, 8), (5, 10, 20), (2, 100, 300)]:\n",
        "    print(f'=== Testing with in_features={in_features} '\n",
        "          f'and out_features={out_features} ===')\n",
        "    input_np = np.random.randn(batch_size, in_features)\n",
        "    output_gradient = np.random.randn(batch_size, out_features)\n",
        "    weight = np.random.randn(out_features, in_features)\n",
        "    bias = np.random.randn(out_features)\n",
        "    customized_linear = CustomizedLinear(in_features, out_features)\n",
        "    customized_linear.set_params({'weight': weight, 'bias': bias})\n",
        "    torch_linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    compare_with_actual_layer(\n",
        "        torch_linear,\n",
        "        customized_linear,\n",
        "        [input_np],\n",
        "        output_gradient,\n",
        "        param_np=[weight, bias],\n",
        "        parameter_names=['weight', 'bias'],\n",
        "    )\n",
        "\n",
        "verify_linear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-CfU1moa3vE"
      },
      "source": [
        "## Question A.4 Back-propagation of Conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW-KNdMea3vF"
      },
      "source": [
        "At last, let us try convolutional layer. To make your life easier, let us assume:\n",
        "* Padding is always 0\n",
        "* No striding\n",
        "* Kernel is always squared\n",
        "\n",
        "Similarly, complete the implementation of the following class below, then run the test below.\n",
        "\n",
        "* For your reference, we have provided the forward pass.\n",
        "* Please complete the backward pass `input_gradients` and `param_gradients`.\n",
        "\n",
        "Also, we have additional requirement for this question:\n",
        "* You cannot use any library functions that can do convolution, either from numpy, pytorch, or other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGnII2-wa3vF"
      },
      "outputs": [],
      "source": [
        "#@title CustomizedConv2d\n",
        "\n",
        "class CustomizedConv2d(CustomizedLayer):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size):\n",
        "    if (kernel_size % 2 == 0):\n",
        "      raise ValueError(f\"Kernel size {kernel_size} must be odd\")\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.weight = np.zeros((out_channels, in_channels, kernel_size, kernel_size))\n",
        "    self.bias = np.zeros(out_channels)\n",
        "\n",
        "  def set_params(self, param_dict: Dict[str, torch.Tensor]):\n",
        "    self.weight = param_dict['weight']\n",
        "    self.bias = param_dict['bias']\n",
        "\n",
        "  def forward(self, inputs: List[np.ndarray]) -> np.ndarray:\n",
        "    input = inputs[0] # [batch_size, in_channels, height, width]\n",
        "    batch_size, in_channels, height, width = input.shape\n",
        "    if self.in_channels != in_channels:\n",
        "      raise ValueError(f'Input channels {in_channels} do not '\n",
        "                       f'match expected {self.in_channels}')\n",
        "    w = self.weight # [out_channels, in_channels, kernel_size, kernel_size]\n",
        "    b = self.bias\n",
        "    out_height = height - self.kernel_size + 1\n",
        "    out_width = width - self.kernel_size + 1\n",
        "    output = np.zeros((batch_size, self.out_channels, out_height, out_width),\n",
        "                      np.float64)\n",
        "    kf = (self.kernel_size - 1) // 2\n",
        "    for dx in range(0, self.kernel_size):\n",
        "      for dy in range(0, self.kernel_size):\n",
        "        # channel order: [B, out_channels, in_channels, H, W]\n",
        "        y_start = dy\n",
        "        y_end = y_start + out_height\n",
        "        x_start = dx\n",
        "        x_end = x_start + out_width\n",
        "        output += (input[:, np.newaxis, :, y_start:y_end, x_start:x_end] *\n",
        "                   w[np.newaxis, :, :, np.newaxis, np.newaxis, dy, dx]\n",
        "        ).sum(axis=2)\n",
        "    output += b[np.newaxis, :, np.newaxis, np.newaxis]\n",
        "    return output\n",
        "\n",
        "  def input_gradients(\n",
        "    self,\n",
        "    inputs: List[np.ndarray],\n",
        "    outputs: np.ndarray,\n",
        "    output_gradient: np.ndarray\n",
        "  ) -> Union[np.ndarray, List[np.ndarray]]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    input = inputs[0]\n",
        "    batch_size, in_channels, height, width = input.shape\n",
        "    input_gradient = np.zeros_like(input)\n",
        "    out_height = height - self.kernel_size + 1\n",
        "    out_width = width - self.kernel_size + 1\n",
        "    for dx in range(0, self.kernel_size):\n",
        "      for dy in range(0, self.kernel_size):\n",
        "        # channel order: [B, out_channels, in_channels, H, W]\n",
        "        y_start = dy\n",
        "        y_end = y_start + out_height\n",
        "        x_start = dx\n",
        "        x_end = x_start + out_width\n",
        "        input_gradient[:, :, y_start:y_end, x_start:x_end] += (\n",
        "            output_gradient[:, :, np.newaxis, :, :] *\n",
        "            self.weight[:, :, dy, dx][np.newaxis, :, :, np.newaxis, np.newaxis]\n",
        "        ).sum(axis=1)\n",
        "    return [input_gradient]\n",
        "\n",
        "  def param_gradients(\n",
        "    self,\n",
        "    inputs: List[np.ndarray],\n",
        "    outputs: np.ndarray,\n",
        "    output_gradient: np.ndarray\n",
        "  ) -> Dict[str, np.ndarray]:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    input = inputs[0]\n",
        "    batch_size, in_channels, height, width = input.shape\n",
        "    out_height = height - self.kernel_size + 1\n",
        "    out_width = width - self.kernel_size + 1\n",
        "    weight_gradient = np.zeros_like(self.weight)\n",
        "    for dx in range(0, self.kernel_size):\n",
        "      for dy in range(0, self.kernel_size):\n",
        "        y_start = dy\n",
        "        y_end = y_start + out_height\n",
        "        x_start = dx\n",
        "        x_end = x_start + out_width\n",
        "        weight_gradient[:, :, dy, dx] += (\n",
        "            input[:, np.newaxis, :, y_start:y_end, x_start:x_end] *\n",
        "            output_gradient[:, :, np.newaxis, :, :]\n",
        "        ).sum(axis=(0, 3, 4))\n",
        "    bias_weight = np.sum(output_gradient, axis=(0, 2, 3))\n",
        "    return {'weight': weight_gradient, 'bias': bias_weight}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eMu7Odta3vF"
      },
      "source": [
        "Run the test below. Do not change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f-_6BeJa3vF",
        "outputId": "5ffdd1e6-8c71-407c-8313-7e9a628a151b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing with in_channels=1 and out_channels=1 and kernel_size=3 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_channels=4 and out_channels=5 and kernel_size=3 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_channels=4 and out_channels=7 and kernel_size=5 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_channels=4 and out_channels=5 and kernel_size=3 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n",
            "=== Testing with in_channels=5 and out_channels=7 and kernel_size=5 ===\n",
            "Output is close to the reference output.\n",
            "Input Gradient 0 is close to the reference output.\n",
            "Parameter Gradient weight is close to the reference output.\n",
            "Parameter Gradient bias is close to the reference output.\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "#@title Run test\n",
        "\n",
        "def verify_conv2d():\n",
        "  for batch_size, in_channels, out_channels, kernel_size, height, width in [\n",
        "      (1, 1, 1, 3, 5, 5),\n",
        "      (1, 4, 5, 3, 11, 11),\n",
        "      (1, 4, 7, 5, 12, 12),\n",
        "      (3, 4, 5, 3, 20, 30),\n",
        "      (1, 5, 7, 5, 256, 256),\n",
        "  ]:\n",
        "    print(f'=== Testing with in_channels={in_channels} '\n",
        "          f'and out_channels={out_channels} and kernel_size={kernel_size} ===')\n",
        "    out_height = height - kernel_size + 1\n",
        "    out_width = width - kernel_size + 1\n",
        "    input_np = np.random.randn(batch_size, in_channels, height, width)\n",
        "    output_gradient = np.random.randn(batch_size, out_channels, out_height, out_width)\n",
        "    weight = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "    bias = np.random.randn(out_channels)\n",
        "    customized_conv2d = CustomizedConv2d(in_channels, out_channels, kernel_size)\n",
        "    customized_conv2d.set_params({'weight': weight, 'bias': bias})\n",
        "    torch_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "\n",
        "    compare_with_actual_layer(\n",
        "        torch_conv2d,\n",
        "        customized_conv2d,\n",
        "        [input_np],\n",
        "        output_gradient,\n",
        "        param_np=[weight, bias],\n",
        "        parameter_names=['weight', 'bias'],\n",
        "    )\n",
        "\n",
        "verify_conv2d()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bZ6FNvhcTz1"
      },
      "source": [
        "Problem B. ResNet\n",
        "=="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znUuc-b-e7Ph"
      },
      "source": [
        "For the ease of network design, we resize the image to 32x32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8lIDrv4-sVt",
        "outputId": "699eb684-4ee7-41fe-d082-ed5777e1db66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:03<00:00, 8.08MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 154kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.86MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 24.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Load the training dataset\n",
        "\n",
        "# Define transformations for the training and test sets\n",
        "\n",
        "mean_train = 0.2860405743122101\n",
        "std_train = 0.3530242443084717\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean_train,), (std_train,))\n",
        "])\n",
        "\n",
        "# The transformation applied to the testing is the same as training\n",
        "transform_test = transforms.Compose([\n",
        "    # For the ease of network design, we resize the image to 32x32[\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean_train,), (std_train,))\n",
        "])\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo54bW0dUWQI"
      },
      "source": [
        "Also, here are the same utility function as assignment 2. If necessary, feel free to change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIFvhPdarN3R"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for training\n",
        "\n",
        "def train_one_epoch(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: str='cpu',\n",
        "    loss_print_iter: int=100\n",
        "  ):\n",
        "  num_train_samples = len(dataloader.dataset)\n",
        "\n",
        "  # Set the model to the training mod\n",
        "  model.train()\n",
        "  all_losses = []\n",
        "  all_acc = []\n",
        "\n",
        "  for batch_index, (image, label) in enumerate(dataloader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    pred = model(image)\n",
        "    loss = loss_fn(pred, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss\n",
        "    all_losses.append(loss.item())\n",
        "    acc = ((pred.argmax(1) == label).type(torch.float).sum().item() /\n",
        "           image.shape[0])\n",
        "    all_acc.append(acc)\n",
        "\n",
        "    if batch_index % loss_print_iter == 0:\n",
        "      loss, trained_samples = loss.item(), (batch_index + 1) * image.shape[0]\n",
        "      print(f'loss: {loss:>7f} '\n",
        "            f'[{trained_samples:>5d}/{num_train_samples:>5d}] ')\n",
        "\n",
        "  return all_losses, all_acc\n",
        "\n",
        "def test_all_samples(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    device: str='cpu'\n",
        ") -> None:\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  num_testing_samples = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Disable gradient calculation for inference\n",
        "  with torch.no_grad():\n",
        "    for image, label in dataloader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      pred = model(image)\n",
        "      test_loss += loss_fn(pred, label).item()\n",
        "      correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  acc = correct / num_testing_samples\n",
        "  print(f'Test Error: \\n Accuracy: {(100*acc):>0.1f}%, '\n",
        "        f'Avg loss: {test_loss:>8f} \\n')\n",
        "  return test_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMq16DScd2Ku"
      },
      "source": [
        "And let us define the running devices.\n",
        "\n",
        "GPU devices are suggested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_kZO4DHd3YE",
        "outputId": "8c275622-a0ac-4713-9c93-af24846e4cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6geKFu_SeBo4"
      },
      "source": [
        "In this question, let us verify why resnet structure is important for deep network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6L3qhRTdCdT"
      },
      "source": [
        "First, let us create the data loader. Feel free to change the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8OcfwZtdAWj"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0_N1wdmeRKE"
      },
      "source": [
        "## Problem B.1 A simple CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ2FxFNleV8h"
      },
      "source": [
        "No coding is required. Just run the following blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1KHZ5cWeU8S"
      },
      "outputs": [],
      "source": [
        "#@title An utility function to create network\n",
        "\n",
        "def gen_double_conv(in_channels, out_channels):\n",
        "  return [nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "          nn.ReLU()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwVlgOiLuB95"
      },
      "source": [
        "First, we will define an CNN networks with 11 convolution/linear layers, and let us try it on FashineMNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQHeQZNpem2a"
      },
      "outputs": [],
      "source": [
        "#@title SimpleCNN\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    img_size = 32,\n",
        "    in_channels = 1,\n",
        "    num_classes = 10,\n",
        "  ):\n",
        "\n",
        "    super().__init__()\n",
        "    base_channel = 16\n",
        "\n",
        "    # Starting block. One 2x downampling.\n",
        "    self.start_conv = nn.Sequential(\n",
        "        *gen_double_conv(1, base_channel),\n",
        "    )\n",
        "\n",
        "    self.start_ds = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "\n",
        "    # Mid block. No downsampling.\n",
        "    self.mid_block = nn.Sequential(\n",
        "        nn.Conv2d(base_channel, base_channel * 2, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.AvgPool2d(2),\n",
        "        nn.Conv2d(base_channel * 2, base_channel * 2, 3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        # We ConvTranspose2d to increase the resolution of an image by 2.\n",
        "        nn.ConvTranspose2d(base_channel * 2, base_channel, 2, stride=2),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    # Last block. Two 2x downsampling.\n",
        "    self.end_conv1 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel, base_channel * 2)\n",
        "    )\n",
        "    self.end_ds1 = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "    self.end_conv2 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel * 2, base_channel * 4)\n",
        "    )\n",
        "\n",
        "    n_pooling_layer = 3\n",
        "    last_layer_size = img_size // (2 ** n_pooling_layer)\n",
        "    self.end_ds2_linear = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(last_layer_size * last_layer_size *\n",
        "                  base_channel * 4, base_channel * 8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(base_channel * 8, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    after_start = self.start_ds(self.start_conv(x))\n",
        "    after_mid = self.mid_block(after_start)\n",
        "    output = self.end_ds2_linear(\n",
        "        self.end_conv2(self.end_ds1(self.end_conv1(after_mid))))\n",
        "    return output\n",
        "\n",
        "  def get_n_conv_linear_layers(self) -> int:\n",
        "    all_blocks = [self.start_conv, self.start_ds,\n",
        "                  self.mid_block,\n",
        "                  self.end_conv1, self.end_ds1,\n",
        "                  self.end_conv2, self.end_ds2_linear]\n",
        "    module_list = (nn.ConvTranspose2d, nn.Conv2d, nn.Linear)\n",
        "    nlayer = 0\n",
        "    for block in all_blocks:\n",
        "      nlayer += len([mod for mod in block.modules()\n",
        "                    if isinstance(mod, module_list)])\n",
        "    return nlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTwEn56psT4Z",
        "outputId": "c81ee8fa-0c69-453a-da6b-253682d15d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model contains 11 conv or linear layers\n"
          ]
        }
      ],
      "source": [
        "#@title Create a model and corresponding optmizer\n",
        "\n",
        "model = SimpleCNN()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 20   # Number of training epochs\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),    # All trainable parameters.\n",
        "  lr=1e-4                # Learning rate\n",
        ")\n",
        "\n",
        "print(f'The model contains {model.get_n_conv_linear_layers()} conv or '\n",
        "      f'linear layers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AlVVuCSsrrC"
      },
      "source": [
        "Running the training below.\n",
        "\n",
        "After training, you should be avaiable to get accuracy of 89% on the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXhOt421saUd",
        "outputId": "c319ea4c-4669-43ba-b81a-0e20a1d10d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.309474 [   32/60000] \n",
            "loss: 2.284549 [ 3232/60000] \n",
            "loss: 1.244536 [ 6432/60000] \n",
            "loss: 1.264719 [ 9632/60000] \n",
            "loss: 0.838336 [12832/60000] \n",
            "loss: 0.664978 [16032/60000] \n",
            "loss: 0.658524 [19232/60000] \n",
            "loss: 0.671320 [22432/60000] \n",
            "loss: 0.725209 [25632/60000] \n",
            "loss: 0.849925 [28832/60000] \n",
            "loss: 0.562990 [32032/60000] \n",
            "loss: 0.556036 [35232/60000] \n",
            "loss: 0.683814 [38432/60000] \n",
            "loss: 0.513558 [41632/60000] \n",
            "loss: 0.832027 [44832/60000] \n",
            "loss: 0.769928 [48032/60000] \n",
            "loss: 0.446838 [51232/60000] \n",
            "loss: 0.703982 [54432/60000] \n",
            "loss: 0.556531 [57632/60000] \n",
            "One epoch takes 25.626473903656006\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.629762 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.552083 [   32/60000] \n",
            "loss: 0.463386 [ 3232/60000] \n",
            "loss: 0.606315 [ 6432/60000] \n",
            "loss: 0.496216 [ 9632/60000] \n",
            "loss: 0.421425 [12832/60000] \n",
            "loss: 0.436497 [16032/60000] \n",
            "loss: 0.591903 [19232/60000] \n",
            "loss: 0.519413 [22432/60000] \n",
            "loss: 0.376786 [25632/60000] \n",
            "loss: 0.464878 [28832/60000] \n",
            "loss: 0.525809 [32032/60000] \n",
            "loss: 0.658475 [35232/60000] \n",
            "loss: 0.662529 [38432/60000] \n",
            "loss: 0.498295 [41632/60000] \n",
            "loss: 0.451899 [44832/60000] \n",
            "loss: 0.549407 [48032/60000] \n",
            "loss: 0.448486 [51232/60000] \n",
            "loss: 0.418736 [54432/60000] \n",
            "loss: 0.777429 [57632/60000] \n",
            "One epoch takes 24.511876583099365\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.532958 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.450734 [   32/60000] \n",
            "loss: 0.544903 [ 3232/60000] \n",
            "loss: 0.347250 [ 6432/60000] \n",
            "loss: 0.615680 [ 9632/60000] \n",
            "loss: 0.376461 [12832/60000] \n",
            "loss: 0.688282 [16032/60000] \n",
            "loss: 0.371957 [19232/60000] \n",
            "loss: 0.708212 [22432/60000] \n",
            "loss: 0.877104 [25632/60000] \n",
            "loss: 0.480776 [28832/60000] \n",
            "loss: 0.575209 [32032/60000] \n",
            "loss: 0.675103 [35232/60000] \n",
            "loss: 0.441366 [38432/60000] \n",
            "loss: 0.359613 [41632/60000] \n",
            "loss: 0.358694 [44832/60000] \n",
            "loss: 0.513438 [48032/60000] \n",
            "loss: 0.418499 [51232/60000] \n",
            "loss: 0.389776 [54432/60000] \n",
            "loss: 0.570140 [57632/60000] \n",
            "One epoch takes 24.212918281555176\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.494003 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.576603 [   32/60000] \n",
            "loss: 0.493001 [ 3232/60000] \n",
            "loss: 0.462146 [ 6432/60000] \n",
            "loss: 0.406846 [ 9632/60000] \n",
            "loss: 0.275363 [12832/60000] \n",
            "loss: 0.414109 [16032/60000] \n",
            "loss: 0.649048 [19232/60000] \n",
            "loss: 0.776021 [22432/60000] \n",
            "loss: 0.487589 [25632/60000] \n",
            "loss: 0.485221 [28832/60000] \n",
            "loss: 0.641709 [32032/60000] \n",
            "loss: 0.384052 [35232/60000] \n",
            "loss: 0.443966 [38432/60000] \n",
            "loss: 0.482089 [41632/60000] \n",
            "loss: 0.304714 [44832/60000] \n",
            "loss: 0.510458 [48032/60000] \n",
            "loss: 0.295045 [51232/60000] \n",
            "loss: 0.370951 [54432/60000] \n",
            "loss: 0.351915 [57632/60000] \n",
            "One epoch takes 24.286279678344727\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.445175 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.391689 [   32/60000] \n",
            "loss: 0.512999 [ 3232/60000] \n",
            "loss: 0.590028 [ 6432/60000] \n",
            "loss: 0.622176 [ 9632/60000] \n",
            "loss: 0.482265 [12832/60000] \n",
            "loss: 0.205504 [16032/60000] \n",
            "loss: 0.396464 [19232/60000] \n",
            "loss: 0.363823 [22432/60000] \n",
            "loss: 0.429808 [25632/60000] \n",
            "loss: 0.206701 [28832/60000] \n",
            "loss: 0.249358 [32032/60000] \n",
            "loss: 0.594028 [35232/60000] \n",
            "loss: 0.373703 [38432/60000] \n",
            "loss: 0.288166 [41632/60000] \n",
            "loss: 0.273627 [44832/60000] \n",
            "loss: 0.202519 [48032/60000] \n",
            "loss: 0.339941 [51232/60000] \n",
            "loss: 0.498213 [54432/60000] \n",
            "loss: 0.422692 [57632/60000] \n",
            "One epoch takes 26.938969373703003\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.398352 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.184292 [   32/60000] \n",
            "loss: 0.178863 [ 3232/60000] \n",
            "loss: 0.276878 [ 6432/60000] \n",
            "loss: 0.142075 [ 9632/60000] \n",
            "loss: 0.197934 [12832/60000] \n",
            "loss: 0.203704 [16032/60000] \n",
            "loss: 0.456023 [19232/60000] \n",
            "loss: 0.172197 [22432/60000] \n",
            "loss: 0.488937 [25632/60000] \n",
            "loss: 0.419200 [28832/60000] \n",
            "loss: 0.175422 [32032/60000] \n",
            "loss: 0.451558 [35232/60000] \n",
            "loss: 0.229284 [38432/60000] \n",
            "loss: 0.397165 [41632/60000] \n",
            "loss: 0.163352 [44832/60000] \n",
            "loss: 0.548324 [48032/60000] \n",
            "loss: 0.353817 [51232/60000] \n",
            "loss: 0.318393 [54432/60000] \n",
            "loss: 0.462824 [57632/60000] \n",
            "One epoch takes 24.506362915039062\n",
            "Test Error: \n",
            " Accuracy: 86.1%, Avg loss: 0.376942 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.528822 [   32/60000] \n",
            "loss: 0.068674 [ 3232/60000] \n",
            "loss: 0.275113 [ 6432/60000] \n",
            "loss: 0.341271 [ 9632/60000] \n",
            "loss: 0.260381 [12832/60000] \n",
            "loss: 0.167874 [16032/60000] \n",
            "loss: 0.369602 [19232/60000] \n",
            "loss: 0.314896 [22432/60000] \n",
            "loss: 0.217031 [25632/60000] \n",
            "loss: 0.310714 [28832/60000] \n",
            "loss: 0.402993 [32032/60000] \n",
            "loss: 0.252175 [35232/60000] \n",
            "loss: 0.181736 [38432/60000] \n",
            "loss: 0.293751 [41632/60000] \n",
            "loss: 0.354081 [44832/60000] \n",
            "loss: 0.310954 [48032/60000] \n",
            "loss: 0.213427 [51232/60000] \n",
            "loss: 0.457343 [54432/60000] \n",
            "loss: 0.146576 [57632/60000] \n",
            "One epoch takes 24.269556045532227\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.368896 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.563783 [   32/60000] \n",
            "loss: 0.326032 [ 3232/60000] \n",
            "loss: 0.166185 [ 6432/60000] \n",
            "loss: 0.270613 [ 9632/60000] \n",
            "loss: 0.238792 [12832/60000] \n",
            "loss: 0.121639 [16032/60000] \n",
            "loss: 0.207562 [19232/60000] \n",
            "loss: 0.398472 [22432/60000] \n",
            "loss: 0.269896 [25632/60000] \n",
            "loss: 0.219889 [28832/60000] \n",
            "loss: 0.147773 [32032/60000] \n",
            "loss: 0.241308 [35232/60000] \n",
            "loss: 0.347272 [38432/60000] \n",
            "loss: 0.158646 [41632/60000] \n",
            "loss: 0.374270 [44832/60000] \n",
            "loss: 0.315409 [48032/60000] \n",
            "loss: 0.491311 [51232/60000] \n",
            "loss: 0.275179 [54432/60000] \n",
            "loss: 0.339069 [57632/60000] \n",
            "One epoch takes 24.2538845539093\n",
            "Test Error: \n",
            " Accuracy: 87.3%, Avg loss: 0.345593 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.331465 [   32/60000] \n",
            "loss: 0.281652 [ 3232/60000] \n",
            "loss: 0.266900 [ 6432/60000] \n",
            "loss: 0.438336 [ 9632/60000] \n",
            "loss: 0.222534 [12832/60000] \n",
            "loss: 0.364644 [16032/60000] \n",
            "loss: 0.329626 [19232/60000] \n",
            "loss: 0.418225 [22432/60000] \n",
            "loss: 0.280373 [25632/60000] \n",
            "loss: 0.288489 [28832/60000] \n",
            "loss: 0.317734 [32032/60000] \n",
            "loss: 0.390125 [35232/60000] \n",
            "loss: 0.380946 [38432/60000] \n",
            "loss: 0.265932 [41632/60000] \n",
            "loss: 0.153227 [44832/60000] \n",
            "loss: 0.387051 [48032/60000] \n",
            "loss: 0.252311 [51232/60000] \n",
            "loss: 0.351233 [54432/60000] \n",
            "loss: 0.273251 [57632/60000] \n",
            "One epoch takes 24.32334303855896\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.325877 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.420234 [   32/60000] \n",
            "loss: 0.303004 [ 3232/60000] \n",
            "loss: 0.611073 [ 6432/60000] \n",
            "loss: 0.427793 [ 9632/60000] \n",
            "loss: 0.187848 [12832/60000] \n",
            "loss: 0.182405 [16032/60000] \n",
            "loss: 0.362609 [19232/60000] \n",
            "loss: 0.294742 [22432/60000] \n",
            "loss: 0.153380 [25632/60000] \n",
            "loss: 0.298964 [28832/60000] \n",
            "loss: 0.272195 [32032/60000] \n",
            "loss: 0.298512 [35232/60000] \n",
            "loss: 0.191309 [38432/60000] \n",
            "loss: 0.231762 [41632/60000] \n",
            "loss: 0.217354 [44832/60000] \n",
            "loss: 0.237938 [48032/60000] \n",
            "loss: 0.430097 [51232/60000] \n",
            "loss: 0.655508 [54432/60000] \n",
            "loss: 0.329441 [57632/60000] \n",
            "One epoch takes 24.43687105178833\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.324033 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.340088 [   32/60000] \n",
            "loss: 0.485363 [ 3232/60000] \n",
            "loss: 0.184121 [ 6432/60000] \n",
            "loss: 0.098971 [ 9632/60000] \n",
            "loss: 0.444433 [12832/60000] \n",
            "loss: 0.228668 [16032/60000] \n",
            "loss: 0.107691 [19232/60000] \n",
            "loss: 0.412592 [22432/60000] \n",
            "loss: 0.315406 [25632/60000] \n",
            "loss: 0.325260 [28832/60000] \n",
            "loss: 0.148231 [32032/60000] \n",
            "loss: 0.377810 [35232/60000] \n",
            "loss: 0.160894 [38432/60000] \n",
            "loss: 0.249231 [41632/60000] \n",
            "loss: 0.155731 [44832/60000] \n",
            "loss: 0.288189 [48032/60000] \n",
            "loss: 0.225509 [51232/60000] \n",
            "loss: 0.248782 [54432/60000] \n",
            "loss: 0.144680 [57632/60000] \n",
            "One epoch takes 24.546878814697266\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.319692 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.117354 [   32/60000] \n",
            "loss: 0.296048 [ 3232/60000] \n",
            "loss: 0.186993 [ 6432/60000] \n",
            "loss: 0.316021 [ 9632/60000] \n",
            "loss: 0.132460 [12832/60000] \n",
            "loss: 0.281319 [16032/60000] \n",
            "loss: 0.095263 [19232/60000] \n",
            "loss: 0.165634 [22432/60000] \n",
            "loss: 0.472948 [25632/60000] \n",
            "loss: 0.698097 [28832/60000] \n",
            "loss: 0.395075 [32032/60000] \n",
            "loss: 0.132118 [35232/60000] \n",
            "loss: 0.250336 [38432/60000] \n",
            "loss: 0.236598 [41632/60000] \n",
            "loss: 0.154878 [44832/60000] \n",
            "loss: 0.276882 [48032/60000] \n",
            "loss: 0.361908 [51232/60000] \n",
            "loss: 0.212671 [54432/60000] \n",
            "loss: 0.508793 [57632/60000] \n",
            "One epoch takes 24.693089962005615\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.299421 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.222072 [   32/60000] \n",
            "loss: 0.197961 [ 3232/60000] \n",
            "loss: 0.256939 [ 6432/60000] \n",
            "loss: 0.171374 [ 9632/60000] \n",
            "loss: 0.202265 [12832/60000] \n",
            "loss: 0.183772 [16032/60000] \n",
            "loss: 0.140888 [19232/60000] \n",
            "loss: 0.245803 [22432/60000] \n",
            "loss: 0.347204 [25632/60000] \n",
            "loss: 0.362886 [28832/60000] \n",
            "loss: 0.105180 [32032/60000] \n",
            "loss: 0.224790 [35232/60000] \n",
            "loss: 0.251039 [38432/60000] \n",
            "loss: 0.310189 [41632/60000] \n",
            "loss: 0.419386 [44832/60000] \n",
            "loss: 0.262262 [48032/60000] \n",
            "loss: 0.415503 [51232/60000] \n",
            "loss: 0.087807 [54432/60000] \n",
            "loss: 0.209895 [57632/60000] \n",
            "One epoch takes 24.29482078552246\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.296114 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.202702 [   32/60000] \n",
            "loss: 0.158247 [ 3232/60000] \n",
            "loss: 0.352333 [ 6432/60000] \n",
            "loss: 0.108408 [ 9632/60000] \n",
            "loss: 0.618022 [12832/60000] \n",
            "loss: 0.341128 [16032/60000] \n",
            "loss: 0.184466 [19232/60000] \n",
            "loss: 0.322533 [22432/60000] \n",
            "loss: 0.322998 [25632/60000] \n",
            "loss: 0.201129 [28832/60000] \n",
            "loss: 0.291529 [32032/60000] \n",
            "loss: 0.213062 [35232/60000] \n",
            "loss: 0.204903 [38432/60000] \n",
            "loss: 0.271538 [41632/60000] \n",
            "loss: 0.318212 [44832/60000] \n",
            "loss: 0.387012 [48032/60000] \n",
            "loss: 0.102855 [51232/60000] \n",
            "loss: 0.185584 [54432/60000] \n",
            "loss: 0.376975 [57632/60000] \n",
            "One epoch takes 24.280391454696655\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.286374 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.091492 [   32/60000] \n",
            "loss: 0.250255 [ 3232/60000] \n",
            "loss: 0.163119 [ 6432/60000] \n",
            "loss: 0.404053 [ 9632/60000] \n",
            "loss: 0.446111 [12832/60000] \n",
            "loss: 0.178330 [16032/60000] \n",
            "loss: 0.105032 [19232/60000] \n",
            "loss: 0.204882 [22432/60000] \n",
            "loss: 0.092548 [25632/60000] \n",
            "loss: 0.264139 [28832/60000] \n",
            "loss: 0.282528 [32032/60000] \n",
            "loss: 0.262273 [35232/60000] \n",
            "loss: 0.192527 [38432/60000] \n",
            "loss: 0.297772 [41632/60000] \n",
            "loss: 0.245044 [44832/60000] \n",
            "loss: 0.393681 [48032/60000] \n",
            "loss: 0.191152 [51232/60000] \n",
            "loss: 0.239196 [54432/60000] \n",
            "loss: 0.225291 [57632/60000] \n",
            "One epoch takes 24.176501274108887\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.285685 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.327754 [   32/60000] \n",
            "loss: 0.249120 [ 3232/60000] \n",
            "loss: 0.182478 [ 6432/60000] \n",
            "loss: 0.404498 [ 9632/60000] \n",
            "loss: 0.411115 [12832/60000] \n",
            "loss: 0.462727 [16032/60000] \n",
            "loss: 0.248642 [19232/60000] \n",
            "loss: 0.171295 [22432/60000] \n",
            "loss: 0.197919 [25632/60000] \n",
            "loss: 0.150002 [28832/60000] \n",
            "loss: 0.356233 [32032/60000] \n",
            "loss: 0.412110 [35232/60000] \n",
            "loss: 0.130735 [38432/60000] \n",
            "loss: 0.172217 [41632/60000] \n",
            "loss: 0.104553 [44832/60000] \n",
            "loss: 0.362282 [48032/60000] \n",
            "loss: 0.266406 [51232/60000] \n",
            "loss: 0.236935 [54432/60000] \n",
            "loss: 0.175988 [57632/60000] \n",
            "One epoch takes 24.120124101638794\n",
            "Test Error: \n",
            " Accuracy: 89.9%, Avg loss: 0.278597 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.046507 [   32/60000] \n",
            "loss: 0.096347 [ 3232/60000] \n",
            "loss: 0.131761 [ 6432/60000] \n",
            "loss: 0.305498 [ 9632/60000] \n",
            "loss: 0.277656 [12832/60000] \n",
            "loss: 0.282402 [16032/60000] \n",
            "loss: 0.291150 [19232/60000] \n",
            "loss: 0.335549 [22432/60000] \n",
            "loss: 0.295970 [25632/60000] \n",
            "loss: 0.194286 [28832/60000] \n",
            "loss: 0.264973 [32032/60000] \n",
            "loss: 0.092762 [35232/60000] \n",
            "loss: 0.306400 [38432/60000] \n",
            "loss: 0.139792 [41632/60000] \n",
            "loss: 0.152402 [44832/60000] \n",
            "loss: 0.102048 [48032/60000] \n",
            "loss: 0.260259 [51232/60000] \n",
            "loss: 0.260089 [54432/60000] \n",
            "loss: 0.232919 [57632/60000] \n",
            "One epoch takes 24.154217004776\n",
            "Test Error: \n",
            " Accuracy: 89.9%, Avg loss: 0.274082 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.069755 [   32/60000] \n",
            "loss: 0.076834 [ 3232/60000] \n",
            "loss: 0.239675 [ 6432/60000] \n",
            "loss: 0.070311 [ 9632/60000] \n",
            "loss: 0.102682 [12832/60000] \n",
            "loss: 0.291618 [16032/60000] \n",
            "loss: 0.100115 [19232/60000] \n",
            "loss: 0.106532 [22432/60000] \n",
            "loss: 0.445655 [25632/60000] \n",
            "loss: 0.367698 [28832/60000] \n",
            "loss: 0.185437 [32032/60000] \n",
            "loss: 0.150313 [35232/60000] \n",
            "loss: 0.045595 [38432/60000] \n",
            "loss: 0.263204 [41632/60000] \n",
            "loss: 0.408837 [44832/60000] \n",
            "loss: 0.182920 [48032/60000] \n",
            "loss: 0.089173 [51232/60000] \n",
            "loss: 0.235309 [54432/60000] \n",
            "loss: 0.342091 [57632/60000] \n",
            "One epoch takes 24.15553855895996\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.277566 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.399130 [   32/60000] \n",
            "loss: 0.203849 [ 3232/60000] \n",
            "loss: 0.367765 [ 6432/60000] \n",
            "loss: 0.213604 [ 9632/60000] \n",
            "loss: 0.200251 [12832/60000] \n",
            "loss: 0.186151 [16032/60000] \n",
            "loss: 0.337434 [19232/60000] \n",
            "loss: 0.241531 [22432/60000] \n",
            "loss: 0.407478 [25632/60000] \n",
            "loss: 0.035455 [28832/60000] \n",
            "loss: 0.204535 [32032/60000] \n",
            "loss: 0.149096 [35232/60000] \n",
            "loss: 0.376473 [38432/60000] \n",
            "loss: 0.247938 [41632/60000] \n",
            "loss: 0.146914 [44832/60000] \n",
            "loss: 0.139426 [48032/60000] \n",
            "loss: 0.075340 [51232/60000] \n",
            "loss: 0.351744 [54432/60000] \n",
            "loss: 0.161656 [57632/60000] \n",
            "One epoch takes 24.036566257476807\n",
            "Test Error: \n",
            " Accuracy: 90.3%, Avg loss: 0.271352 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.432954 [   32/60000] \n",
            "loss: 0.091127 [ 3232/60000] \n",
            "loss: 0.504513 [ 6432/60000] \n",
            "loss: 0.417878 [ 9632/60000] \n",
            "loss: 0.033552 [12832/60000] \n",
            "loss: 0.234950 [16032/60000] \n",
            "loss: 0.261181 [19232/60000] \n",
            "loss: 0.108041 [22432/60000] \n",
            "loss: 0.185907 [25632/60000] \n",
            "loss: 0.234703 [28832/60000] \n",
            "loss: 0.336720 [32032/60000] \n",
            "loss: 0.140209 [35232/60000] \n",
            "loss: 0.151564 [38432/60000] \n",
            "loss: 0.179474 [41632/60000] \n",
            "loss: 0.169882 [44832/60000] \n",
            "loss: 0.374361 [48032/60000] \n",
            "loss: 0.225235 [51232/60000] \n",
            "loss: 0.232207 [54432/60000] \n",
            "loss: 0.203409 [57632/60000] \n",
            "One epoch takes 23.970872163772583\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.288606 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.288606 \n",
            "\n",
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "#@title Run training\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  start_time =time.time()\n",
        "  _, _ = train_one_epoch(train_loader, model, loss_fn, optimizer, device=device)\n",
        "  print(f'One epoch takes {time.time() - start_time}')\n",
        "  _, _ = test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "print(\"Training done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4In_-2bEtNLd"
      },
      "source": [
        "## Problem B.2. Deeper CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofod1d7WttHs"
      },
      "source": [
        "Now, please change the network we have provided, by duplicating some of conv parts. For example, you can make a multiple copy of self.mid_block. The network you have created should have at least 30 conv/linear layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv9Em2dUtPm2"
      },
      "outputs": [],
      "source": [
        "class DeeperCNN(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    img_size=32,\n",
        "    in_channels=1,\n",
        "    num_classes=10,\n",
        "  ):\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    super().__init__()\n",
        "    base_channel = 16\n",
        "\n",
        "    # Starting block. One 2x downampling.\n",
        "    self.start_conv = nn.Sequential(\n",
        "        *gen_double_conv(1, base_channel),\n",
        "        *gen_double_conv(base_channel, base_channel),\n",
        "    )\n",
        "\n",
        "    self.start_ds = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "\n",
        "    mid_blocks = []\n",
        "    for _ in range(5):\n",
        "        mid_blocks.append(self._make_mid_block(base_channel))\n",
        "\n",
        "    self.deepmid = nn.Sequential(*mid_blocks)\n",
        "\n",
        "    # Last block. Two 2x downsampling.\n",
        "    self.end_conv1 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel, base_channel * 2),\n",
        "        *gen_double_conv(base_channel * 2, base_channel * 2),\n",
        "    )\n",
        "    self.end_ds1 = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "    self.end_conv2 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel * 2, base_channel * 4),\n",
        "        *gen_double_conv(base_channel * 4, base_channel * 4)\n",
        "    )\n",
        "\n",
        "    n_pooling_layer = 3\n",
        "    last_layer_size = img_size // (2 ** n_pooling_layer)\n",
        "    self.end_ds2_linear = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(last_layer_size * last_layer_size *\n",
        "                  base_channel * 4, base_channel * 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(base_channel * 16, base_channel * 8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(base_channel * 8, num_classes)\n",
        "    )\n",
        "\n",
        "  def _make_mid_block(self, base_channel):\n",
        "        # Mid block. No downsampling.\n",
        "      return nn.Sequential(\n",
        "            nn.Conv2d(base_channel, base_channel * 2, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Conv2d(base_channel * 2, base_channel * 2, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "           # We ConvTranspose2d to increase the resolution of an image by 2.\n",
        "            nn.ConvTranspose2d(base_channel * 2, base_channel, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    after_start = self.start_ds(self.start_conv(x))\n",
        "    after_mid = self.deepmid(after_start)\n",
        "    output = self.end_ds2_linear(\n",
        "    self.end_conv2(self.end_ds1(self.end_conv1(after_mid))))\n",
        "    return output\n",
        "\n",
        "  def get_n_conv_linear_layers(self) -> int:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    all_blocks = [self.start_conv, self.start_ds,\n",
        "                  self.deepmid,\n",
        "                  self.end_conv1, self.end_ds1,\n",
        "                  self.end_conv2, self.end_ds2_linear]\n",
        "    module_list = (nn.ConvTranspose2d, nn.Conv2d, nn.Linear)\n",
        "    nlayer = 0\n",
        "    for block in all_blocks:\n",
        "      nlayer += len([mod for mod in block.modules()\n",
        "                    if isinstance(mod, module_list)])\n",
        "    return nlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMLRGwZ9vPmm",
        "outputId": "f53dbfcd-f151-4a38-958c-957626bedc87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model contains 30 conv or linear layers\n"
          ]
        }
      ],
      "source": [
        "#@title Create a model and corresponding optmizer\n",
        "\n",
        "model = DeeperCNN()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 20   # Number of training epochs\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),    # All trainable parameters.\n",
        "  lr=1e-4,     # Learning rate\n",
        ")\n",
        "\n",
        "print(f'The model contains {model.get_n_conv_linear_layers()} conv or '\n",
        "      f'linear layers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS2O4oLqvPmm"
      },
      "source": [
        "Running the training below.\n",
        "\n",
        "Interesting, you will find that with deeper network, the accuracy actually goes down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nODMxe3bvPmm",
        "outputId": "59ede5c5-d3ae-4054-d357-fe16ce339398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301057 [   32/60000] \n",
            "loss: 2.303802 [ 3232/60000] \n",
            "loss: 2.298149 [ 6432/60000] \n",
            "loss: 2.307278 [ 9632/60000] \n",
            "loss: 2.303458 [12832/60000] \n",
            "loss: 2.304575 [16032/60000] \n",
            "loss: 2.303729 [19232/60000] \n",
            "loss: 2.306975 [22432/60000] \n",
            "loss: 2.300779 [25632/60000] \n",
            "loss: 2.298641 [28832/60000] \n",
            "loss: 2.304524 [32032/60000] \n",
            "loss: 2.297641 [35232/60000] \n",
            "loss: 2.301429 [38432/60000] \n",
            "loss: 2.307261 [41632/60000] \n",
            "loss: 2.300042 [44832/60000] \n",
            "loss: 2.304665 [48032/60000] \n",
            "loss: 2.303902 [51232/60000] \n",
            "loss: 2.302754 [54432/60000] \n",
            "loss: 2.301396 [57632/60000] \n",
            "One epoch takes 30.39143991470337\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302640 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301344 [   32/60000] \n",
            "loss: 2.301189 [ 3232/60000] \n",
            "loss: 2.303574 [ 6432/60000] \n",
            "loss: 2.302982 [ 9632/60000] \n",
            "loss: 2.304130 [12832/60000] \n",
            "loss: 2.302846 [16032/60000] \n",
            "loss: 2.303244 [19232/60000] \n",
            "loss: 2.300157 [22432/60000] \n",
            "loss: 2.298765 [25632/60000] \n",
            "loss: 2.300865 [28832/60000] \n",
            "loss: 2.306514 [32032/60000] \n",
            "loss: 2.299947 [35232/60000] \n",
            "loss: 2.300710 [38432/60000] \n",
            "loss: 2.306721 [41632/60000] \n",
            "loss: 2.298529 [44832/60000] \n",
            "loss: 2.300091 [48032/60000] \n",
            "loss: 2.302963 [51232/60000] \n",
            "loss: 2.301620 [54432/60000] \n",
            "loss: 2.302441 [57632/60000] \n",
            "One epoch takes 30.650940418243408\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302633 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.300918 [   32/60000] \n",
            "loss: 2.307714 [ 3232/60000] \n",
            "loss: 2.307359 [ 6432/60000] \n",
            "loss: 2.300017 [ 9632/60000] \n",
            "loss: 2.305781 [12832/60000] \n",
            "loss: 2.301303 [16032/60000] \n",
            "loss: 2.300945 [19232/60000] \n",
            "loss: 2.302230 [22432/60000] \n",
            "loss: 2.303653 [25632/60000] \n",
            "loss: 2.303781 [28832/60000] \n",
            "loss: 2.304003 [32032/60000] \n",
            "loss: 2.307245 [35232/60000] \n",
            "loss: 2.302481 [38432/60000] \n",
            "loss: 2.304967 [41632/60000] \n",
            "loss: 2.303527 [44832/60000] \n",
            "loss: 2.303021 [48032/60000] \n",
            "loss: 2.302158 [51232/60000] \n",
            "loss: 2.301937 [54432/60000] \n",
            "loss: 2.302703 [57632/60000] \n",
            "One epoch takes 30.604665279388428\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302591 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.304207 [   32/60000] \n",
            "loss: 2.302960 [ 3232/60000] \n",
            "loss: 2.303955 [ 6432/60000] \n",
            "loss: 2.301938 [ 9632/60000] \n",
            "loss: 2.299955 [12832/60000] \n",
            "loss: 2.302125 [16032/60000] \n",
            "loss: 2.302984 [19232/60000] \n",
            "loss: 2.302187 [22432/60000] \n",
            "loss: 2.302232 [25632/60000] \n",
            "loss: 2.304711 [28832/60000] \n",
            "loss: 2.306312 [32032/60000] \n",
            "loss: 2.300935 [35232/60000] \n",
            "loss: 2.301800 [38432/60000] \n",
            "loss: 2.301778 [41632/60000] \n",
            "loss: 2.300580 [44832/60000] \n",
            "loss: 2.304522 [48032/60000] \n",
            "loss: 2.298900 [51232/60000] \n",
            "loss: 2.302661 [54432/60000] \n",
            "loss: 2.303370 [57632/60000] \n",
            "One epoch takes 31.329634189605713\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302627 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.304524 [   32/60000] \n",
            "loss: 2.302278 [ 3232/60000] \n",
            "loss: 2.301033 [ 6432/60000] \n",
            "loss: 2.304797 [ 9632/60000] \n",
            "loss: 2.300727 [12832/60000] \n",
            "loss: 2.302076 [16032/60000] \n",
            "loss: 2.302063 [19232/60000] \n",
            "loss: 2.303521 [22432/60000] \n",
            "loss: 2.302304 [25632/60000] \n",
            "loss: 2.302914 [28832/60000] \n",
            "loss: 2.301923 [32032/60000] \n",
            "loss: 2.300416 [35232/60000] \n",
            "loss: 2.301433 [38432/60000] \n",
            "loss: 2.304097 [41632/60000] \n",
            "loss: 2.304511 [44832/60000] \n",
            "loss: 2.302945 [48032/60000] \n",
            "loss: 2.302028 [51232/60000] \n",
            "loss: 2.302238 [54432/60000] \n",
            "loss: 2.304278 [57632/60000] \n",
            "One epoch takes 32.95611238479614\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302597 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.301032 [   32/60000] \n",
            "loss: 2.299551 [ 3232/60000] \n",
            "loss: 2.302313 [ 6432/60000] \n",
            "loss: 2.302994 [ 9632/60000] \n",
            "loss: 2.301955 [12832/60000] \n",
            "loss: 2.302938 [16032/60000] \n",
            "loss: 2.299916 [19232/60000] \n",
            "loss: 2.299539 [22432/60000] \n",
            "loss: 2.304819 [25632/60000] \n",
            "loss: 2.303531 [28832/60000] \n",
            "loss: 2.302979 [32032/60000] \n",
            "loss: 2.302691 [35232/60000] \n",
            "loss: 2.300945 [38432/60000] \n",
            "loss: 2.302833 [41632/60000] \n",
            "loss: 2.302456 [44832/60000] \n",
            "loss: 2.303394 [48032/60000] \n",
            "loss: 2.303492 [51232/60000] \n",
            "loss: 2.302721 [54432/60000] \n",
            "loss: 2.301365 [57632/60000] \n",
            "One epoch takes 30.565571784973145\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302601 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.302207 [   32/60000] \n",
            "loss: 2.303164 [ 3232/60000] \n",
            "loss: 2.300770 [ 6432/60000] \n",
            "loss: 2.301066 [ 9632/60000] \n",
            "loss: 2.302608 [12832/60000] \n",
            "loss: 2.301915 [16032/60000] \n",
            "loss: 2.301947 [19232/60000] \n",
            "loss: 2.301986 [22432/60000] \n",
            "loss: 2.301996 [25632/60000] \n",
            "loss: 2.301793 [28832/60000] \n",
            "loss: 2.303632 [32032/60000] \n",
            "loss: 2.302762 [35232/60000] \n",
            "loss: 2.302710 [38432/60000] \n",
            "loss: 2.302379 [41632/60000] \n",
            "loss: 2.302716 [44832/60000] \n",
            "loss: 2.302564 [48032/60000] \n",
            "loss: 2.302051 [51232/60000] \n",
            "loss: 2.302254 [54432/60000] \n",
            "loss: 2.300987 [57632/60000] \n",
            "One epoch takes 30.545363187789917\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302590 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.302047 [   32/60000] \n",
            "loss: 2.302710 [ 3232/60000] \n",
            "loss: 2.302863 [ 6432/60000] \n",
            "loss: 2.302382 [ 9632/60000] \n",
            "loss: 2.300766 [12832/60000] \n",
            "loss: 2.304349 [16032/60000] \n",
            "loss: 2.302168 [19232/60000] \n",
            "loss: 2.302005 [22432/60000] \n",
            "loss: 2.304602 [25632/60000] \n",
            "loss: 2.302342 [28832/60000] \n",
            "loss: 2.301118 [32032/60000] \n",
            "loss: 2.302199 [35232/60000] \n",
            "loss: 2.304382 [38432/60000] \n",
            "loss: 2.301673 [41632/60000] \n",
            "loss: 2.303249 [44832/60000] \n",
            "loss: 2.303110 [48032/60000] \n",
            "loss: 2.302250 [51232/60000] \n",
            "loss: 2.302999 [54432/60000] \n",
            "loss: 2.302728 [57632/60000] \n",
            "One epoch takes 30.227611780166626\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302588 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.302232 [   32/60000] \n",
            "loss: 2.302668 [ 3232/60000] \n",
            "loss: 2.304179 [ 6432/60000] \n",
            "loss: 2.303408 [ 9632/60000] \n",
            "loss: 2.301712 [12832/60000] \n",
            "loss: 2.303727 [16032/60000] \n",
            "loss: 2.303257 [19232/60000] \n",
            "loss: 2.303755 [22432/60000] \n",
            "loss: 2.302840 [25632/60000] \n",
            "loss: 2.305071 [28832/60000] \n",
            "loss: 2.303377 [32032/60000] \n",
            "loss: 2.302139 [35232/60000] \n",
            "loss: 2.303911 [38432/60000] \n",
            "loss: 2.303091 [41632/60000] \n",
            "loss: 2.303201 [44832/60000] \n",
            "loss: 2.303037 [48032/60000] \n",
            "loss: 2.301720 [51232/60000] \n",
            "loss: 2.301890 [54432/60000] \n",
            "loss: 2.303849 [57632/60000] \n",
            "One epoch takes 30.8181734085083\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302590 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.301705 [   32/60000] \n",
            "loss: 2.302651 [ 3232/60000] \n",
            "loss: 2.301621 [ 6432/60000] \n",
            "loss: 2.302436 [ 9632/60000] \n",
            "loss: 2.304264 [12832/60000] \n",
            "loss: 2.302491 [16032/60000] \n",
            "loss: 2.302765 [19232/60000] \n",
            "loss: 2.302035 [22432/60000] \n",
            "loss: 2.302594 [25632/60000] \n",
            "loss: 2.302835 [28832/60000] \n",
            "loss: 2.301598 [32032/60000] \n",
            "loss: 2.302124 [35232/60000] \n",
            "loss: 2.302441 [38432/60000] \n",
            "loss: 2.303436 [41632/60000] \n",
            "loss: 2.304114 [44832/60000] \n",
            "loss: 2.302073 [48032/60000] \n",
            "loss: 2.303156 [51232/60000] \n",
            "loss: 2.305720 [54432/60000] \n",
            "loss: 2.302583 [57632/60000] \n",
            "One epoch takes 32.150545597076416\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302587 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.302630 [   32/60000] \n",
            "loss: 2.301850 [ 3232/60000] \n",
            "loss: 2.303912 [ 6432/60000] \n",
            "loss: 2.303163 [ 9632/60000] \n",
            "loss: 2.301986 [12832/60000] \n",
            "loss: 2.304185 [16032/60000] \n",
            "loss: 2.303834 [19232/60000] \n",
            "loss: 2.302249 [22432/60000] \n",
            "loss: 2.303334 [25632/60000] \n",
            "loss: 2.302755 [28832/60000] \n",
            "loss: 2.302885 [32032/60000] \n",
            "loss: 2.303227 [35232/60000] \n",
            "loss: 2.303828 [38432/60000] \n",
            "loss: 2.301969 [41632/60000] \n",
            "loss: 2.302649 [44832/60000] \n",
            "loss: 2.304201 [48032/60000] \n",
            "loss: 2.302567 [51232/60000] \n",
            "loss: 2.302557 [54432/60000] \n",
            "loss: 2.302705 [57632/60000] \n",
            "One epoch takes 37.28249764442444\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302595 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.303087 [   32/60000] \n",
            "loss: 2.302873 [ 3232/60000] \n",
            "loss: 2.302860 [ 6432/60000] \n",
            "loss: 2.303006 [ 9632/60000] \n",
            "loss: 2.302581 [12832/60000] \n",
            "loss: 2.303615 [16032/60000] \n",
            "loss: 2.302496 [19232/60000] \n",
            "loss: 2.302826 [22432/60000] \n",
            "loss: 2.301511 [25632/60000] \n",
            "loss: 2.302248 [28832/60000] \n",
            "loss: 2.303413 [32032/60000] \n",
            "loss: 2.302144 [35232/60000] \n",
            "loss: 2.304061 [38432/60000] \n",
            "loss: 2.303300 [41632/60000] \n",
            "loss: 2.301636 [44832/60000] \n",
            "loss: 2.303549 [48032/60000] \n",
            "loss: 2.303629 [51232/60000] \n",
            "loss: 2.301814 [54432/60000] \n",
            "loss: 2.303222 [57632/60000] \n",
            "One epoch takes 33.10866069793701\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302589 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.303163 [   32/60000] \n",
            "loss: 2.302373 [ 3232/60000] \n",
            "loss: 2.301844 [ 6432/60000] \n",
            "loss: 2.302950 [ 9632/60000] \n",
            "loss: 2.303081 [12832/60000] \n",
            "loss: 2.303832 [16032/60000] \n",
            "loss: 2.301854 [19232/60000] \n",
            "loss: 2.302791 [22432/60000] \n",
            "loss: 2.303693 [25632/60000] \n",
            "loss: 2.301626 [28832/60000] \n",
            "loss: 2.302507 [32032/60000] \n",
            "loss: 2.302863 [35232/60000] \n",
            "loss: 2.302617 [38432/60000] \n",
            "loss: 2.303231 [41632/60000] \n",
            "loss: 2.301278 [44832/60000] \n",
            "loss: 2.303181 [48032/60000] \n",
            "loss: 2.302001 [51232/60000] \n",
            "loss: 2.302407 [54432/60000] \n",
            "loss: 2.302560 [57632/60000] \n",
            "One epoch takes 32.575923681259155\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302592 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.303510 [   32/60000] \n",
            "loss: 2.303385 [ 3232/60000] \n",
            "loss: 2.303159 [ 6432/60000] \n",
            "loss: 2.302695 [ 9632/60000] \n",
            "loss: 2.303058 [12832/60000] \n",
            "loss: 2.301724 [16032/60000] \n",
            "loss: 2.301947 [19232/60000] \n",
            "loss: 2.301557 [22432/60000] \n",
            "loss: 2.302503 [25632/60000] \n",
            "loss: 2.301305 [28832/60000] \n",
            "loss: 2.302780 [32032/60000] \n",
            "loss: 2.302466 [35232/60000] \n",
            "loss: 2.302089 [38432/60000] \n",
            "loss: 2.303586 [41632/60000] \n",
            "loss: 2.301095 [44832/60000] \n",
            "loss: 2.302880 [48032/60000] \n",
            "loss: 2.301690 [51232/60000] \n",
            "loss: 2.302886 [54432/60000] \n",
            "loss: 2.301958 [57632/60000] \n",
            "One epoch takes 33.62123703956604\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302593 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.303303 [   32/60000] \n",
            "loss: 2.302421 [ 3232/60000] \n",
            "loss: 2.303172 [ 6432/60000] \n",
            "loss: 2.303571 [ 9632/60000] \n",
            "loss: 2.302725 [12832/60000] \n",
            "loss: 2.302520 [16032/60000] \n",
            "loss: 2.303640 [19232/60000] \n",
            "loss: 2.302953 [22432/60000] \n",
            "loss: 2.302809 [25632/60000] \n",
            "loss: 2.302934 [28832/60000] \n",
            "loss: 2.303314 [32032/60000] \n",
            "loss: 2.301909 [35232/60000] \n",
            "loss: 2.302603 [38432/60000] \n",
            "loss: 2.302996 [41632/60000] \n",
            "loss: 2.302837 [44832/60000] \n",
            "loss: 2.303818 [48032/60000] \n",
            "loss: 2.303703 [51232/60000] \n",
            "loss: 2.303637 [54432/60000] \n",
            "loss: 2.303407 [57632/60000] \n",
            "One epoch takes 30.939075231552124\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302592 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.301857 [   32/60000] \n",
            "loss: 2.303004 [ 3232/60000] \n",
            "loss: 2.302626 [ 6432/60000] \n",
            "loss: 2.303754 [ 9632/60000] \n",
            "loss: 2.304851 [12832/60000] \n",
            "loss: 2.302950 [16032/60000] \n",
            "loss: 2.302323 [19232/60000] \n",
            "loss: 2.302126 [22432/60000] \n",
            "loss: 2.302834 [25632/60000] \n",
            "loss: 2.302130 [28832/60000] \n",
            "loss: 2.303385 [32032/60000] \n",
            "loss: 2.302972 [35232/60000] \n",
            "loss: 2.301999 [38432/60000] \n",
            "loss: 2.302356 [41632/60000] \n",
            "loss: 2.302871 [44832/60000] \n",
            "loss: 2.302597 [48032/60000] \n",
            "loss: 2.302692 [51232/60000] \n",
            "loss: 2.302190 [54432/60000] \n",
            "loss: 2.303044 [57632/60000] \n",
            "One epoch takes 30.737112283706665\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302596 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.301811 [   32/60000] \n",
            "loss: 2.303330 [ 3232/60000] \n",
            "loss: 2.302672 [ 6432/60000] \n",
            "loss: 2.303703 [ 9632/60000] \n",
            "loss: 2.302424 [12832/60000] \n",
            "loss: 2.302892 [16032/60000] \n",
            "loss: 2.303894 [19232/60000] \n",
            "loss: 2.304057 [22432/60000] \n",
            "loss: 2.302419 [25632/60000] \n",
            "loss: 2.302597 [28832/60000] \n",
            "loss: 2.301789 [32032/60000] \n",
            "loss: 2.302957 [35232/60000] \n",
            "loss: 2.303101 [38432/60000] \n",
            "loss: 2.302865 [41632/60000] \n",
            "loss: 2.302820 [44832/60000] \n",
            "loss: 2.302294 [48032/60000] \n",
            "loss: 2.302835 [51232/60000] \n",
            "loss: 2.302665 [54432/60000] \n",
            "loss: 2.304153 [57632/60000] \n",
            "One epoch takes 30.12764549255371\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302588 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.303316 [   32/60000] \n",
            "loss: 2.302784 [ 3232/60000] \n",
            "loss: 2.302679 [ 6432/60000] \n",
            "loss: 2.302796 [ 9632/60000] \n",
            "loss: 2.303807 [12832/60000] \n",
            "loss: 2.302874 [16032/60000] \n",
            "loss: 2.303621 [19232/60000] \n",
            "loss: 2.302011 [22432/60000] \n",
            "loss: 2.302544 [25632/60000] \n",
            "loss: 2.303888 [28832/60000] \n",
            "loss: 2.302986 [32032/60000] \n",
            "loss: 2.301312 [35232/60000] \n",
            "loss: 2.302739 [38432/60000] \n",
            "loss: 2.303112 [41632/60000] \n",
            "loss: 2.302939 [44832/60000] \n",
            "loss: 2.302742 [48032/60000] \n",
            "loss: 2.301694 [51232/60000] \n",
            "loss: 2.302032 [54432/60000] \n",
            "loss: 2.302155 [57632/60000] \n",
            "One epoch takes 29.8925998210907\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302593 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.302708 [   32/60000] \n",
            "loss: 2.302842 [ 3232/60000] \n",
            "loss: 2.302235 [ 6432/60000] \n",
            "loss: 2.302360 [ 9632/60000] \n",
            "loss: 2.301483 [12832/60000] \n",
            "loss: 2.301989 [16032/60000] \n",
            "loss: 2.299829 [19232/60000] \n",
            "loss: 2.302458 [22432/60000] \n",
            "loss: 2.301693 [25632/60000] \n",
            "loss: 2.303535 [28832/60000] \n",
            "loss: 2.301755 [32032/60000] \n",
            "loss: 2.302236 [35232/60000] \n",
            "loss: 2.302947 [38432/60000] \n",
            "loss: 2.301888 [41632/60000] \n",
            "loss: 2.301983 [44832/60000] \n",
            "loss: 2.303625 [48032/60000] \n",
            "loss: 2.302924 [51232/60000] \n",
            "loss: 2.303168 [54432/60000] \n",
            "loss: 2.302703 [57632/60000] \n",
            "One epoch takes 29.980809926986694\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302591 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.302431 [   32/60000] \n",
            "loss: 2.302331 [ 3232/60000] \n",
            "loss: 2.303370 [ 6432/60000] \n",
            "loss: 2.302754 [ 9632/60000] \n",
            "loss: 2.302522 [12832/60000] \n",
            "loss: 2.301174 [16032/60000] \n",
            "loss: 2.302448 [19232/60000] \n",
            "loss: 2.303938 [22432/60000] \n",
            "loss: 2.300841 [25632/60000] \n",
            "loss: 2.302689 [28832/60000] \n",
            "loss: 2.301879 [32032/60000] \n",
            "loss: 2.302437 [35232/60000] \n",
            "loss: 2.302616 [38432/60000] \n",
            "loss: 2.302188 [41632/60000] \n",
            "loss: 2.301908 [44832/60000] \n",
            "loss: 2.302946 [48032/60000] \n",
            "loss: 2.303365 [51232/60000] \n",
            "loss: 2.301969 [54432/60000] \n",
            "loss: 2.302189 [57632/60000] \n",
            "One epoch takes 30.64444875717163\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302590 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302590 \n",
            "\n",
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "#@title Run training\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  start_time =time.time()\n",
        "  _, _ = train_one_epoch(train_loader, model, loss_fn, optimizer, device=device)\n",
        "  print(f'One epoch takes {time.time() - start_time}')\n",
        "  _, _ = test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "print(\"Training done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke2PHKtnvpkp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK6YV0Z3vqMe"
      },
      "source": [
        "## Problem B.3. ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlMwP1MxvqMe"
      },
      "source": [
        "Now, keep the general structure in B.2 almost no change, but add some skip connection (resnet structure). And try to achieve 89% accuracy again.\n",
        "\n",
        "Hint, to define a resnet, when you run the module in the `forward`, instead call it as:\n",
        "```\n",
        "y = net1(x)\n",
        "```\n",
        "Call it as:\n",
        "```\n",
        "y = net1(x) + x\n",
        "```\n",
        "\n",
        "The only requirement is that the input and output of `net1` should be same dimension. For example, you can `self.mid_block` in the SimpleCNN can add a skip connection. Try to find whereelse you can add this skip connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4oHoNKnvqMe"
      },
      "outputs": [],
      "source": [
        "class ResCNN(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    img_size=32,\n",
        "    in_channels=1,\n",
        "    num_classes=10,\n",
        "  ):\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    super().__init__()\n",
        "    base_channel = 16\n",
        "\n",
        "        # Starting block. One 2x downampling.\n",
        "    self.start_conv = nn.Sequential(\n",
        "        *gen_double_conv(1, base_channel),\n",
        "        *gen_double_conv(base_channel, base_channel),\n",
        "    )\n",
        "\n",
        "    self.start_ds = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "\n",
        "    mid_blocks = []\n",
        "    for _ in range(5):\n",
        "        mid_blocks.append(self._make_mid_block(base_channel))\n",
        "\n",
        "    self.deepmid = nn.Sequential(*mid_blocks)\n",
        "\n",
        "    # Last block. Two 2x downsampling.\n",
        "    self.end_conv1 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel, base_channel * 2),\n",
        "        *gen_double_conv(base_channel * 2, base_channel * 2),\n",
        "    )\n",
        "    self.end_ds1 = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "    )\n",
        "    self.end_conv2 = nn.Sequential(\n",
        "        *gen_double_conv(base_channel * 2, base_channel * 4),\n",
        "        *gen_double_conv(base_channel * 4, base_channel * 4)\n",
        "    )\n",
        "\n",
        "    n_pooling_layer = 3\n",
        "    last_layer_size = img_size // (2 ** n_pooling_layer)\n",
        "    self.end_ds2_linear = nn.Sequential(\n",
        "        nn.AvgPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(last_layer_size * last_layer_size *\n",
        "                  base_channel * 4, base_channel * 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(base_channel * 16, base_channel * 8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(base_channel * 8, num_classes)\n",
        "    )\n",
        "\n",
        "  def _make_mid_block(self, base_channel):\n",
        "        # Mid block. No downsampling.\n",
        "      return nn.Sequential(\n",
        "            nn.Conv2d(base_channel, base_channel * 2, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Conv2d(base_channel * 2, base_channel * 2, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "           # We ConvTranspose2d to increase the resolution of an image by 2.\n",
        "            nn.ConvTranspose2d(base_channel * 2, base_channel, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    after_start = self.start_ds(self.start_conv(x))\n",
        "    after_mid = self.deepmid(after_start) + after_start\n",
        "    output = self.end_ds2_linear(\n",
        "    self.end_conv2(self.end_ds1(self.end_conv1(after_mid))))\n",
        "    return output\n",
        "\n",
        "  def get_n_conv_linear_layers(self) -> int:\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    all_blocks = [self.start_conv, self.start_ds,\n",
        "                  self.deepmid,\n",
        "                  self.end_conv1, self.end_ds1,\n",
        "                  self.end_conv2, self.end_ds2_linear]\n",
        "    module_list = (nn.ConvTranspose2d, nn.Conv2d, nn.Linear)\n",
        "    nlayer = 0\n",
        "    for block in all_blocks:\n",
        "      nlayer += len([mod for mod in block.modules()\n",
        "                    if isinstance(mod, module_list)])\n",
        "    return nlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_R4GBi7vqMe",
        "outputId": "9c8cdc22-f292-4968-b585-76fedbc2bc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model contains 30 conv or linear layers\n"
          ]
        }
      ],
      "source": [
        "#@title Create a model and corresponding optmizer\n",
        "\n",
        "model = ResCNN()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 20   # Number of training epochs\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),    # All trainable parameters.\n",
        "  lr=1e-4                # Learning rate\n",
        ")\n",
        "\n",
        "print(f'The model contains {model.get_n_conv_linear_layers()} conv or '\n",
        "      f'linear layers')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPIYaqQxvqMe"
      },
      "source": [
        "Running the training below.\n",
        "\n",
        "Now, with resnet design, you will should reach 89% again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktZyblIFvqMe",
        "outputId": "90f1ef69-cb90-4f95-b279-7b0f23e10f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.292154 [   32/60000] \n",
            "loss: 2.290864 [ 3232/60000] \n",
            "loss: 0.812189 [ 6432/60000] \n",
            "loss: 0.828353 [ 9632/60000] \n",
            "loss: 0.847551 [12832/60000] \n",
            "loss: 0.729506 [16032/60000] \n",
            "loss: 0.762317 [19232/60000] \n",
            "loss: 0.454517 [22432/60000] \n",
            "loss: 0.544702 [25632/60000] \n",
            "loss: 0.844015 [28832/60000] \n",
            "loss: 0.700097 [32032/60000] \n",
            "loss: 0.773231 [35232/60000] \n",
            "loss: 0.516823 [38432/60000] \n",
            "loss: 0.714197 [41632/60000] \n",
            "loss: 0.734287 [44832/60000] \n",
            "loss: 0.598461 [48032/60000] \n",
            "loss: 0.630968 [51232/60000] \n",
            "loss: 0.743768 [54432/60000] \n",
            "loss: 0.718938 [57632/60000] \n",
            "One epoch takes 30.364739656448364\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.616047 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.786737 [   32/60000] \n",
            "loss: 0.782411 [ 3232/60000] \n",
            "loss: 0.387957 [ 6432/60000] \n",
            "loss: 0.642474 [ 9632/60000] \n",
            "loss: 0.811866 [12832/60000] \n",
            "loss: 0.370436 [16032/60000] \n",
            "loss: 0.713840 [19232/60000] \n",
            "loss: 0.510155 [22432/60000] \n",
            "loss: 0.475609 [25632/60000] \n",
            "loss: 0.532214 [28832/60000] \n",
            "loss: 0.487790 [32032/60000] \n",
            "loss: 0.650178 [35232/60000] \n",
            "loss: 0.638472 [38432/60000] \n",
            "loss: 0.519011 [41632/60000] \n",
            "loss: 0.487046 [44832/60000] \n",
            "loss: 0.717557 [48032/60000] \n",
            "loss: 0.504382 [51232/60000] \n",
            "loss: 0.441594 [54432/60000] \n",
            "loss: 0.378569 [57632/60000] \n",
            "One epoch takes 30.230862855911255\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.503061 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.415770 [   32/60000] \n",
            "loss: 0.328439 [ 3232/60000] \n",
            "loss: 0.325396 [ 6432/60000] \n",
            "loss: 0.702187 [ 9632/60000] \n",
            "loss: 0.573654 [12832/60000] \n",
            "loss: 0.160293 [16032/60000] \n",
            "loss: 0.332820 [19232/60000] \n",
            "loss: 0.486480 [22432/60000] \n",
            "loss: 0.217744 [25632/60000] \n",
            "loss: 0.287473 [28832/60000] \n",
            "loss: 0.416560 [32032/60000] \n",
            "loss: 0.422113 [35232/60000] \n",
            "loss: 0.444286 [38432/60000] \n",
            "loss: 0.758987 [41632/60000] \n",
            "loss: 0.655373 [44832/60000] \n",
            "loss: 0.417118 [48032/60000] \n",
            "loss: 0.637538 [51232/60000] \n",
            "loss: 0.384880 [54432/60000] \n",
            "loss: 0.287307 [57632/60000] \n",
            "One epoch takes 31.529210090637207\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.436986 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.373711 [   32/60000] \n",
            "loss: 0.533256 [ 3232/60000] \n",
            "loss: 0.515431 [ 6432/60000] \n",
            "loss: 0.466222 [ 9632/60000] \n",
            "loss: 0.385375 [12832/60000] \n",
            "loss: 0.637556 [16032/60000] \n",
            "loss: 0.375058 [19232/60000] \n",
            "loss: 0.110469 [22432/60000] \n",
            "loss: 0.457589 [25632/60000] \n",
            "loss: 0.437803 [28832/60000] \n",
            "loss: 0.250335 [32032/60000] \n",
            "loss: 0.568715 [35232/60000] \n",
            "loss: 0.378160 [38432/60000] \n",
            "loss: 0.335147 [41632/60000] \n",
            "loss: 0.341119 [44832/60000] \n",
            "loss: 0.323821 [48032/60000] \n",
            "loss: 0.460934 [51232/60000] \n",
            "loss: 0.602256 [54432/60000] \n",
            "loss: 0.645391 [57632/60000] \n",
            "One epoch takes 30.980974435806274\n",
            "Test Error: \n",
            " Accuracy: 85.4%, Avg loss: 0.381477 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.313977 [   32/60000] \n",
            "loss: 0.287369 [ 3232/60000] \n",
            "loss: 0.311900 [ 6432/60000] \n",
            "loss: 0.161492 [ 9632/60000] \n",
            "loss: 0.533855 [12832/60000] \n",
            "loss: 0.220287 [16032/60000] \n",
            "loss: 0.672354 [19232/60000] \n",
            "loss: 0.328876 [22432/60000] \n",
            "loss: 0.357892 [25632/60000] \n",
            "loss: 0.218525 [28832/60000] \n",
            "loss: 0.222015 [32032/60000] \n",
            "loss: 0.326395 [35232/60000] \n",
            "loss: 0.394366 [38432/60000] \n",
            "loss: 0.324712 [41632/60000] \n",
            "loss: 0.372124 [44832/60000] \n",
            "loss: 0.379656 [48032/60000] \n",
            "loss: 0.358430 [51232/60000] \n",
            "loss: 0.273331 [54432/60000] \n",
            "loss: 0.253261 [57632/60000] \n",
            "One epoch takes 30.60227108001709\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.358525 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.183190 [   32/60000] \n",
            "loss: 0.370608 [ 3232/60000] \n",
            "loss: 0.275843 [ 6432/60000] \n",
            "loss: 0.488161 [ 9632/60000] \n",
            "loss: 0.318576 [12832/60000] \n",
            "loss: 0.157596 [16032/60000] \n",
            "loss: 0.414078 [19232/60000] \n",
            "loss: 0.268591 [22432/60000] \n",
            "loss: 0.094036 [25632/60000] \n",
            "loss: 0.207908 [28832/60000] \n",
            "loss: 0.398106 [32032/60000] \n",
            "loss: 0.289098 [35232/60000] \n",
            "loss: 0.583721 [38432/60000] \n",
            "loss: 0.352580 [41632/60000] \n",
            "loss: 0.303368 [44832/60000] \n",
            "loss: 0.400023 [48032/60000] \n",
            "loss: 0.330649 [51232/60000] \n",
            "loss: 0.274605 [54432/60000] \n",
            "loss: 0.364681 [57632/60000] \n",
            "One epoch takes 30.086360931396484\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.338043 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.119860 [   32/60000] \n",
            "loss: 0.630296 [ 3232/60000] \n",
            "loss: 0.389159 [ 6432/60000] \n",
            "loss: 0.296681 [ 9632/60000] \n",
            "loss: 0.393852 [12832/60000] \n",
            "loss: 0.343973 [16032/60000] \n",
            "loss: 0.460544 [19232/60000] \n",
            "loss: 0.193625 [22432/60000] \n",
            "loss: 0.186466 [25632/60000] \n",
            "loss: 0.549199 [28832/60000] \n",
            "loss: 0.486390 [32032/60000] \n",
            "loss: 0.404854 [35232/60000] \n",
            "loss: 0.240877 [38432/60000] \n",
            "loss: 0.206382 [41632/60000] \n",
            "loss: 0.139423 [44832/60000] \n",
            "loss: 0.274120 [48032/60000] \n",
            "loss: 0.198341 [51232/60000] \n",
            "loss: 0.270876 [54432/60000] \n",
            "loss: 0.317459 [57632/60000] \n",
            "One epoch takes 30.123979568481445\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.329598 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.153408 [   32/60000] \n",
            "loss: 0.373561 [ 3232/60000] \n",
            "loss: 0.257289 [ 6432/60000] \n",
            "loss: 0.175192 [ 9632/60000] \n",
            "loss: 0.342758 [12832/60000] \n",
            "loss: 0.158253 [16032/60000] \n",
            "loss: 0.498109 [19232/60000] \n",
            "loss: 0.299532 [22432/60000] \n",
            "loss: 0.247726 [25632/60000] \n",
            "loss: 0.297197 [28832/60000] \n",
            "loss: 0.449404 [32032/60000] \n",
            "loss: 0.438452 [35232/60000] \n",
            "loss: 0.097590 [38432/60000] \n",
            "loss: 0.548912 [41632/60000] \n",
            "loss: 0.195076 [44832/60000] \n",
            "loss: 0.176723 [48032/60000] \n",
            "loss: 0.217942 [51232/60000] \n",
            "loss: 0.165161 [54432/60000] \n",
            "loss: 0.341195 [57632/60000] \n",
            "One epoch takes 30.81596350669861\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.298757 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.131124 [   32/60000] \n",
            "loss: 0.244386 [ 3232/60000] \n",
            "loss: 0.281083 [ 6432/60000] \n",
            "loss: 0.401231 [ 9632/60000] \n",
            "loss: 0.289634 [12832/60000] \n",
            "loss: 0.158018 [16032/60000] \n",
            "loss: 0.158819 [19232/60000] \n",
            "loss: 0.145561 [22432/60000] \n",
            "loss: 0.246952 [25632/60000] \n",
            "loss: 0.144355 [28832/60000] \n",
            "loss: 0.473208 [32032/60000] \n",
            "loss: 0.329107 [35232/60000] \n",
            "loss: 0.229908 [38432/60000] \n",
            "loss: 0.217090 [41632/60000] \n",
            "loss: 0.431291 [44832/60000] \n",
            "loss: 0.258809 [48032/60000] \n",
            "loss: 0.048042 [51232/60000] \n",
            "loss: 0.476737 [54432/60000] \n",
            "loss: 0.209830 [57632/60000] \n",
            "One epoch takes 30.747820615768433\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.301933 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.250387 [   32/60000] \n",
            "loss: 0.098862 [ 3232/60000] \n",
            "loss: 0.308954 [ 6432/60000] \n",
            "loss: 0.149751 [ 9632/60000] \n",
            "loss: 0.368886 [12832/60000] \n",
            "loss: 0.070934 [16032/60000] \n",
            "loss: 0.207156 [19232/60000] \n",
            "loss: 0.153415 [22432/60000] \n",
            "loss: 0.343408 [25632/60000] \n",
            "loss: 0.307596 [28832/60000] \n",
            "loss: 0.353424 [32032/60000] \n",
            "loss: 0.168378 [35232/60000] \n",
            "loss: 0.204144 [38432/60000] \n",
            "loss: 0.189448 [41632/60000] \n",
            "loss: 0.203163 [44832/60000] \n",
            "loss: 0.107817 [48032/60000] \n",
            "loss: 0.397824 [51232/60000] \n",
            "loss: 0.158199 [54432/60000] \n",
            "loss: 0.241466 [57632/60000] \n",
            "One epoch takes 30.84188461303711\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.280664 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.181853 [   32/60000] \n",
            "loss: 0.492468 [ 3232/60000] \n",
            "loss: 0.485731 [ 6432/60000] \n",
            "loss: 0.191475 [ 9632/60000] \n",
            "loss: 0.237758 [12832/60000] \n",
            "loss: 0.207386 [16032/60000] \n",
            "loss: 0.347322 [19232/60000] \n",
            "loss: 0.147798 [22432/60000] \n",
            "loss: 0.234888 [25632/60000] \n",
            "loss: 0.108696 [28832/60000] \n",
            "loss: 0.246163 [32032/60000] \n",
            "loss: 0.267821 [35232/60000] \n",
            "loss: 0.170253 [38432/60000] \n",
            "loss: 0.375150 [41632/60000] \n",
            "loss: 0.233916 [44832/60000] \n",
            "loss: 0.558398 [48032/60000] \n",
            "loss: 0.121813 [51232/60000] \n",
            "loss: 0.164743 [54432/60000] \n",
            "loss: 0.068513 [57632/60000] \n",
            "One epoch takes 30.376691102981567\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.290389 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.492779 [   32/60000] \n",
            "loss: 0.490402 [ 3232/60000] \n",
            "loss: 0.283479 [ 6432/60000] \n",
            "loss: 0.155113 [ 9632/60000] \n",
            "loss: 0.259734 [12832/60000] \n",
            "loss: 0.287134 [16032/60000] \n",
            "loss: 0.304754 [19232/60000] \n",
            "loss: 0.185348 [22432/60000] \n",
            "loss: 0.194053 [25632/60000] \n",
            "loss: 0.145362 [28832/60000] \n",
            "loss: 0.206415 [32032/60000] \n",
            "loss: 0.289069 [35232/60000] \n",
            "loss: 0.162106 [38432/60000] \n",
            "loss: 0.318661 [41632/60000] \n",
            "loss: 0.133808 [44832/60000] \n",
            "loss: 0.229833 [48032/60000] \n",
            "loss: 0.178286 [51232/60000] \n",
            "loss: 0.412091 [54432/60000] \n",
            "loss: 0.166255 [57632/60000] \n",
            "One epoch takes 30.2319815158844\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.270141 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.254845 [   32/60000] \n",
            "loss: 0.166250 [ 3232/60000] \n",
            "loss: 0.277949 [ 6432/60000] \n",
            "loss: 0.369811 [ 9632/60000] \n",
            "loss: 0.267446 [12832/60000] \n",
            "loss: 0.316249 [16032/60000] \n",
            "loss: 0.310595 [19232/60000] \n",
            "loss: 0.223880 [22432/60000] \n",
            "loss: 0.137607 [25632/60000] \n",
            "loss: 0.154596 [28832/60000] \n",
            "loss: 0.285252 [32032/60000] \n",
            "loss: 0.155783 [35232/60000] \n",
            "loss: 0.219393 [38432/60000] \n",
            "loss: 0.331529 [41632/60000] \n",
            "loss: 0.177196 [44832/60000] \n",
            "loss: 0.374294 [48032/60000] \n",
            "loss: 0.177751 [51232/60000] \n",
            "loss: 0.256482 [54432/60000] \n",
            "loss: 0.162661 [57632/60000] \n",
            "One epoch takes 30.684671878814697\n",
            "Test Error: \n",
            " Accuracy: 90.3%, Avg loss: 0.266787 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.222881 [   32/60000] \n",
            "loss: 0.056001 [ 3232/60000] \n",
            "loss: 0.410836 [ 6432/60000] \n",
            "loss: 0.124373 [ 9632/60000] \n",
            "loss: 0.141918 [12832/60000] \n",
            "loss: 0.093202 [16032/60000] \n",
            "loss: 0.294438 [19232/60000] \n",
            "loss: 0.143316 [22432/60000] \n",
            "loss: 0.059031 [25632/60000] \n",
            "loss: 0.040694 [28832/60000] \n",
            "loss: 0.104922 [32032/60000] \n",
            "loss: 0.175214 [35232/60000] \n",
            "loss: 0.069175 [38432/60000] \n",
            "loss: 0.247156 [41632/60000] \n",
            "loss: 0.182737 [44832/60000] \n",
            "loss: 0.225915 [48032/60000] \n",
            "loss: 0.263361 [51232/60000] \n",
            "loss: 0.112434 [54432/60000] \n",
            "loss: 0.164978 [57632/60000] \n",
            "One epoch takes 30.778860330581665\n",
            "Test Error: \n",
            " Accuracy: 90.1%, Avg loss: 0.275834 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.163716 [   32/60000] \n",
            "loss: 0.361124 [ 3232/60000] \n",
            "loss: 0.218489 [ 6432/60000] \n",
            "loss: 0.309583 [ 9632/60000] \n",
            "loss: 0.062405 [12832/60000] \n",
            "loss: 0.195066 [16032/60000] \n",
            "loss: 0.136918 [19232/60000] \n",
            "loss: 0.394480 [22432/60000] \n",
            "loss: 0.128133 [25632/60000] \n",
            "loss: 0.081663 [28832/60000] \n",
            "loss: 0.100128 [32032/60000] \n",
            "loss: 0.184431 [35232/60000] \n",
            "loss: 0.152666 [38432/60000] \n",
            "loss: 0.257876 [41632/60000] \n",
            "loss: 0.254926 [44832/60000] \n",
            "loss: 0.257733 [48032/60000] \n",
            "loss: 0.076289 [51232/60000] \n",
            "loss: 0.272727 [54432/60000] \n",
            "loss: 0.233984 [57632/60000] \n",
            "One epoch takes 30.758070945739746\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.255631 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.282009 [   32/60000] \n",
            "loss: 0.176875 [ 3232/60000] \n",
            "loss: 0.222279 [ 6432/60000] \n",
            "loss: 0.562043 [ 9632/60000] \n",
            "loss: 0.324475 [12832/60000] \n",
            "loss: 0.126061 [16032/60000] \n",
            "loss: 0.197541 [19232/60000] \n",
            "loss: 0.181811 [22432/60000] \n",
            "loss: 0.268181 [25632/60000] \n",
            "loss: 0.214728 [28832/60000] \n",
            "loss: 0.117062 [32032/60000] \n",
            "loss: 0.360534 [35232/60000] \n",
            "loss: 0.195467 [38432/60000] \n",
            "loss: 0.099719 [41632/60000] \n",
            "loss: 0.182683 [44832/60000] \n",
            "loss: 0.150396 [48032/60000] \n",
            "loss: 0.111896 [51232/60000] \n",
            "loss: 0.126225 [54432/60000] \n",
            "loss: 0.131009 [57632/60000] \n",
            "One epoch takes 30.62744903564453\n",
            "Test Error: \n",
            " Accuracy: 90.7%, Avg loss: 0.255622 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.115234 [   32/60000] \n",
            "loss: 0.187269 [ 3232/60000] \n",
            "loss: 0.068580 [ 6432/60000] \n",
            "loss: 0.357314 [ 9632/60000] \n",
            "loss: 0.107006 [12832/60000] \n",
            "loss: 0.281202 [16032/60000] \n",
            "loss: 0.339874 [19232/60000] \n",
            "loss: 0.211908 [22432/60000] \n",
            "loss: 0.512057 [25632/60000] \n",
            "loss: 0.136127 [28832/60000] \n",
            "loss: 0.267452 [32032/60000] \n",
            "loss: 0.150792 [35232/60000] \n",
            "loss: 0.261317 [38432/60000] \n",
            "loss: 0.131404 [41632/60000] \n",
            "loss: 0.277896 [44832/60000] \n",
            "loss: 0.075862 [48032/60000] \n",
            "loss: 0.131816 [51232/60000] \n",
            "loss: 0.382455 [54432/60000] \n",
            "loss: 0.165078 [57632/60000] \n",
            "One epoch takes 30.271764993667603\n",
            "Test Error: \n",
            " Accuracy: 91.1%, Avg loss: 0.252737 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.097365 [   32/60000] \n",
            "loss: 0.192155 [ 3232/60000] \n",
            "loss: 0.113773 [ 6432/60000] \n",
            "loss: 0.158924 [ 9632/60000] \n",
            "loss: 0.074753 [12832/60000] \n",
            "loss: 0.156079 [16032/60000] \n",
            "loss: 0.299889 [19232/60000] \n",
            "loss: 0.140489 [22432/60000] \n",
            "loss: 0.217448 [25632/60000] \n",
            "loss: 0.019301 [28832/60000] \n",
            "loss: 0.099553 [32032/60000] \n",
            "loss: 0.053141 [35232/60000] \n",
            "loss: 0.150847 [38432/60000] \n",
            "loss: 0.148490 [41632/60000] \n",
            "loss: 0.246089 [44832/60000] \n",
            "loss: 0.047697 [48032/60000] \n",
            "loss: 0.394832 [51232/60000] \n",
            "loss: 0.218211 [54432/60000] \n",
            "loss: 0.199396 [57632/60000] \n",
            "One epoch takes 30.211167573928833\n",
            "Test Error: \n",
            " Accuracy: 91.3%, Avg loss: 0.250085 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.068753 [   32/60000] \n",
            "loss: 0.208521 [ 3232/60000] \n",
            "loss: 0.204001 [ 6432/60000] \n",
            "loss: 0.242177 [ 9632/60000] \n",
            "loss: 0.120746 [12832/60000] \n",
            "loss: 0.204059 [16032/60000] \n",
            "loss: 0.068672 [19232/60000] \n",
            "loss: 0.150138 [22432/60000] \n",
            "loss: 0.172804 [25632/60000] \n",
            "loss: 0.183332 [28832/60000] \n",
            "loss: 0.077655 [32032/60000] \n",
            "loss: 0.161649 [35232/60000] \n",
            "loss: 0.079847 [38432/60000] \n",
            "loss: 0.183231 [41632/60000] \n",
            "loss: 0.242619 [44832/60000] \n",
            "loss: 0.185054 [48032/60000] \n",
            "loss: 0.375509 [51232/60000] \n",
            "loss: 0.047174 [54432/60000] \n",
            "loss: 0.079420 [57632/60000] \n",
            "One epoch takes 31.032544374465942\n",
            "Test Error: \n",
            " Accuracy: 90.7%, Avg loss: 0.263561 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.208351 [   32/60000] \n",
            "loss: 0.160594 [ 3232/60000] \n",
            "loss: 0.225549 [ 6432/60000] \n",
            "loss: 0.097304 [ 9632/60000] \n",
            "loss: 0.240034 [12832/60000] \n",
            "loss: 0.075522 [16032/60000] \n",
            "loss: 0.146873 [19232/60000] \n",
            "loss: 0.085001 [22432/60000] \n",
            "loss: 0.123606 [25632/60000] \n",
            "loss: 0.201138 [28832/60000] \n",
            "loss: 0.232354 [32032/60000] \n",
            "loss: 0.241142 [35232/60000] \n",
            "loss: 0.069829 [38432/60000] \n",
            "loss: 0.216596 [41632/60000] \n",
            "loss: 0.261658 [44832/60000] \n",
            "loss: 0.206484 [48032/60000] \n",
            "loss: 0.239583 [51232/60000] \n",
            "loss: 0.159060 [54432/60000] \n",
            "loss: 0.319474 [57632/60000] \n",
            "One epoch takes 31.471357822418213\n",
            "Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.284024 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.284024 \n",
            "\n",
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "#@title Run training\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  start_time =time.time()\n",
        "  _, _ = train_one_epoch(train_loader, model, loss_fn, optimizer, device=device)\n",
        "  print(f'One epoch takes {time.time() - start_time}')\n",
        "  _, _ = test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "print(\"Training done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
