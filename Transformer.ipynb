{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5M8ECGxKIjF"
      },
      "source": [
        "First, let us import all necessary libraries for this one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yDe9uGBwNOdW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import math\n",
        "import time\n",
        "from typing import List, Union, Dict\n",
        "import scipy\n",
        "import scipy.special"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyS1bmkE-BXB"
      },
      "source": [
        "Same utility function for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lju-21LuXbxx"
      },
      "source": [
        "Also, in this one, we need to access a local storage. Please create a Google driver folder and enter their path below. It will ask you to grant permission to colab to access your google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23tFG6oDXi_Y",
        "outputId": "d2c51d11-085e-4217-f03b-4caaa5fce016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google drive (you need to grant colab to access your Google Drive)\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "srcpath = 'cuhk_courses/aims5702'  #@param {type:\"string\"}\n",
        "srcpath = os.path.join('/content/drive/My Drive', srcpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS0XV_ANN8MP"
      },
      "source": [
        "In this one, we will build transformer, for both image classification and text processing. Note that GPU training is highly suggested, as it is much faster than CPU training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNR5h7w1ODqA"
      },
      "source": [
        "First, let us still using the FashinMNIST dataset we used before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGKUNhe5OKVC"
      },
      "source": [
        "# Problem A. Transformer on FashionMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rz65tsEOXmx"
      },
      "source": [
        "Again, let us load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8lIDrv4-sVt",
        "outputId": "da2fd676-79dc-40a7-8d8b-5ebd30df4e8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.79MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 30.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Load the training dataset\n",
        "\n",
        "# Define transformations for the training and test sets\n",
        "\n",
        "mean_train = 0.2860405743122101\n",
        "std_train = 0.3530242443084717\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean_train,), (std_train,))\n",
        "])\n",
        "\n",
        "# The transformation applied to the testing is the same as training\n",
        "transform_test = transforms.Compose([\n",
        "    # For the ease of network design, we resize the image to 32x32[\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean_train,), (std_train,))\n",
        "])\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Y791jioE6e"
      },
      "source": [
        "## Transformer Model functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0kzc8ZOfl9"
      },
      "source": [
        "Although, we can build a transformer on raw pixels, that is often slow in training. Instead, let us first downsample the input image into small size through a lightweight CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdR5AC12Papi"
      },
      "source": [
        "Given an input image with resolution of `1(channel)x32(height)x32(width)`, design a one-layer network to convert that into `768(embedding_dim)x8(height)x8(width)`. It basically downsamples the input images by 4x. Then flatten it into a `768(embedding_dim)x64(num_patches)`\n",
        "\n",
        "**Hint:**\n",
        "* You can achieve this through a stirde=4 and kernel=4 convolution.\n",
        "* You can use `torch.flatten(x, start_dim, end_dim)` to flatten 2D image into 1D sequence.\n",
        "* You can use `torch.transpose` to switch two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9OrglSFUjl0"
      },
      "outputs": [],
      "source": [
        "#@title Problem A.1. Patching embedding class\n",
        "\n",
        "def power_base_10000(x):\n",
        "  \"\"\"\n",
        "  Computes the power of 10000.0 to the input tensor.\n",
        "\n",
        "  Args:\n",
        "    x: The input tensor.\n",
        "  \"\"\"\n",
        "  return torch.exp(math.log(10000.0) * x)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"\n",
        "  A module to convert an image into a sequence of patches and embed them.\n",
        "  This is the first layer of a Vision Transformer.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size, patch_size=4, in_channels=1, embedding_dim=768):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      img_size: The size of the image.\n",
        "      patch_size: The size of the patch.\n",
        "      in_channels: The number of channels in the image.\n",
        "      embedding_dim: The dimension of the embedding.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    self.downsample = nn.Conv2d(\n",
        "        in_channels = in_channels,\n",
        "        out_channels = embedding_dim,\n",
        "        kernel_size = patch_size,\n",
        "        stride = patch_size)\n",
        "    self.projection = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: The input tensor with shape (Batch_Size, In_Channels, Height, Width).\n",
        "\n",
        "    Returns:\n",
        "      The output tensor with shape (Batch_Size, Embedding_Dim, Number_of_Patches).\n",
        "    \"\"\"\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    x = self.downsample(x)\n",
        "    output = x.flatten(2)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vclTjvJ5Tq0"
      },
      "source": [
        "Also, let us define the position encoding.\n",
        "\n",
        "As discussed in the class, we can precompute a position embedding table:\n",
        "\n",
        "`position_encoding_table[t, d]` is the `t` is the position of a token, and `d` is the feature dimension. Let us just use the definition in the class:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CSbEsPj6DpS"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNWZ_ex9UZ_G"
      },
      "outputs": [],
      "source": [
        "#@title Problem A.2. PositionalEncoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"\n",
        "  Adds positional information to the patch embeddings.\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, max_seq_length=512):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      embedding_dim: The dimension of the embedding.\n",
        "      max_seq_length: The maximum sequence length.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    position_encoding_table = torch.zeros(max_seq_length, embedding_dim)\n",
        "    # Create a position table.\n",
        "    # position_encoding_table[i, :] should be length embedding_dim vector that\n",
        "    # encodes the position i.\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    w_k = power_base_10000(torch.arange(0, embedding_dim, 2)/embedding_dim)\n",
        "    position_encoding_table[:, 0::2] = torch.sin(position * w_k)\n",
        "    position_encoding_table[:, 1::2] = torch.cos(position * w_k)\n",
        "    self.register_buffer('position_encoding_table', position_encoding_table)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: The input tensor with shape\n",
        "          (Batch_Size, Sequence_Length, Embedding_Dim).\n",
        "\n",
        "    Returns:\n",
        "      The output tensor with shape (Batch_Size, Sequence_Length, Embedding_Dim).\n",
        "    \"\"\"\n",
        "    _, seq_length, _ = x.shape\n",
        "    return x + self.position_encoding_table[np.newaxis, :seq_length, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOs-HnYkUayX"
      },
      "outputs": [],
      "source": [
        "#@title Problem A.3. ViTClassifier\n",
        "\n",
        "# Transformer class\n",
        "class ViTClassifier(nn.Module):\n",
        "  \"\"\"\n",
        "  Complete Vision Transformer model for classification.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size=32, patch_size=4, in_channels=1, num_classes=10,\n",
        "               embedding_dim=768, num_heads=12, num_layers=6, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # First, let us create the patch embedding and positional encoding layers.\n",
        "    self.patch_embedding = PatchEmbedding(\n",
        "        img_size, patch_size, in_channels, embedding_dim\n",
        "    )\n",
        "    self.positional_encoding = PositionalEncoding(\n",
        "        embedding_dim, (img_size // patch_size)**2\n",
        "    )\n",
        "\n",
        "    # Transformer Encoder layers\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=embedding_dim,\n",
        "        nhead=num_heads,\n",
        "        dropout=dropout_rate,\n",
        "        batch_first=True\n",
        "    )\n",
        "    self.transformer_encoder = nn.TransformerEncoder(\n",
        "        encoder_layer, num_layers=num_layers\n",
        "    )\n",
        "\n",
        "    # This is the classification token CLS.\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
        "\n",
        "    # This is the MLP head for classification.\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(embedding_dim),\n",
        "        nn.Linear(embedding_dim, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: The input tensor with shape (Batch_Size, In_Channels, Height, Width).\n",
        "\n",
        "    Returns:\n",
        "      The output tensor with shape (Batch_Size, Num_Classes).\n",
        "    \"\"\"\n",
        "    # 1. Convert image to patches\n",
        "    # (Batch_Size, In_Channels, Height, Width) ->\n",
        "    #     (Batch_Size, H_p * W_p, Embedding_Dim)\n",
        "    # H_p = Height // Patch_Size\n",
        "    # W_p = Width // Patch_Size\n",
        "    batch_size, _, _, _ = x.shape\n",
        "    patches_embedded = self.patch_embedding(x).transpose(1, 2)\n",
        "\n",
        "    # 2. Add positional information\n",
        "    # (Batch_Size, H_p * W_p, Embedding_Dim) ->\n",
        "    #     (Batch_Size, H_p * W_p, Embedding_Dim)\n",
        "    patches_with_pos = self.positional_encoding(patches_embedded)\n",
        "\n",
        "    # 3. Concatenate the learnable CLS token\n",
        "    # After concatenating, the shape of the input tokens should be\n",
        "    # (Batch_Size, H_p * W_p + 1, Embedding_Dim)\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
        "    input_tokens_extended = torch.concatenate([cls_token, patches_with_pos],\n",
        "                                              dim=1)\n",
        "\n",
        "    # 4. Pass through the Transformer Encoder\n",
        "    output_tokens = self.transformer_encoder(input_tokens_extended)\n",
        "\n",
        "    # 5. Pass the CLS token through the MLP head for final classification\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    cls_token = output_tokens[:, 0, :]\n",
        "    output = self.mlp_head(cls_token)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_YNl4k93j2"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHV0on5k6sgl"
      },
      "source": [
        "Similar, let us use GPU this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AKzZK7O6sHh",
        "outputId": "6e2c209f-fb71-4673-cc82-08441bb47a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5mXfiSC6Zta"
      },
      "source": [
        "At last, uses the same training and testing loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2PJ7bRr6-w8"
      },
      "source": [
        "At last, defines the dataloader, model, and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIW3aTbK7DYn"
      },
      "outputs": [],
      "source": [
        "#@title Dataloader, model, and optimizer\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = ViTClassifier()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5   # Number of training epochs\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),    # All trainable parameters.\n",
        "  lr=1e-4                # Learning rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sVsBgu_PvwG"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for training\n",
        "\n",
        "def train_one_epoch(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: str='cpu',\n",
        "    loss_print_iter: int=100\n",
        "  ):\n",
        "  num_train_samples = len(dataloader.dataset)\n",
        "\n",
        "  # Set the model to the training mod\n",
        "  model.train()\n",
        "  all_losses = []\n",
        "  all_acc = []\n",
        "\n",
        "  for batch_index, (image, label) in enumerate(dataloader):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    pred = model(image)\n",
        "    loss = loss_fn(pred, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss\n",
        "    all_losses.append(loss.item())\n",
        "    acc = ((pred.argmax(1) == label).type(torch.float).sum().item() /\n",
        "           image.shape[0])\n",
        "    all_acc.append(acc)\n",
        "\n",
        "    if batch_index % loss_print_iter == 0:\n",
        "      loss, trained_samples = loss.item(), (batch_index + 1) * image.shape[0]\n",
        "      print(f'loss: {loss:>7f} '\n",
        "            f'[{trained_samples:>5d}/{num_train_samples:>5d}] ')\n",
        "\n",
        "  return all_losses, all_acc\n",
        "\n",
        "def test_all_samples(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    device: str='cpu'\n",
        ") -> None:\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  num_testing_samples = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Disable gradient calculation for inference\n",
        "  with torch.no_grad():\n",
        "    for image, label in dataloader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      pred = model(image)\n",
        "      test_loss += loss_fn(pred, label).item()\n",
        "      correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  acc = correct / num_testing_samples\n",
        "  print(f'Test Error: \\n Accuracy: {(100*acc):>0.1f}%, '\n",
        "        f'Avg loss: {test_loss:>8f} \\n')\n",
        "  return test_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brgff9Gk6nEw"
      },
      "source": [
        "At last, let us try to train it.\n",
        "\n",
        "If your model is defined correctly, you should get 87-88% accuracy.\n",
        "\n",
        "The training may take about 20-30min on T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qbHxmKO6pHj",
        "outputId": "b34f17ff-b2a4-4480-e3fa-3139b33cf0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.166112 [   32/60000] \n",
            "loss: 1.233310 [ 3232/60000] \n",
            "loss: 0.569527 [ 6432/60000] \n",
            "loss: 0.486533 [ 9632/60000] \n",
            "loss: 0.730586 [12832/60000] \n",
            "loss: 0.445954 [16032/60000] \n",
            "loss: 0.454513 [19232/60000] \n",
            "loss: 0.255805 [22432/60000] \n",
            "loss: 0.376292 [25632/60000] \n",
            "loss: 0.396322 [28832/60000] \n",
            "loss: 0.471332 [32032/60000] \n",
            "loss: 0.411718 [35232/60000] \n",
            "loss: 0.173303 [38432/60000] \n",
            "loss: 0.360600 [41632/60000] \n",
            "loss: 0.302020 [44832/60000] \n",
            "loss: 0.544018 [48032/60000] \n",
            "loss: 0.675721 [51232/60000] \n",
            "loss: 0.699524 [54432/60000] \n",
            "loss: 0.333553 [57632/60000] \n",
            "One epoch takes 284.2578411102295\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.480922 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.541045 [   32/60000] \n",
            "loss: 0.161244 [ 3232/60000] \n",
            "loss: 0.267530 [ 6432/60000] \n",
            "loss: 0.310473 [ 9632/60000] \n",
            "loss: 0.483137 [12832/60000] \n",
            "loss: 0.106652 [16032/60000] \n",
            "loss: 0.409042 [19232/60000] \n",
            "loss: 0.278235 [22432/60000] \n",
            "loss: 0.626870 [25632/60000] \n",
            "loss: 0.447881 [28832/60000] \n",
            "loss: 0.381553 [32032/60000] \n",
            "loss: 0.255396 [35232/60000] \n",
            "loss: 0.378138 [38432/60000] \n",
            "loss: 0.433438 [41632/60000] \n",
            "loss: 0.335189 [44832/60000] \n",
            "loss: 0.514458 [48032/60000] \n",
            "loss: 0.350083 [51232/60000] \n",
            "loss: 0.445084 [54432/60000] \n",
            "loss: 0.183616 [57632/60000] \n",
            "One epoch takes 285.6817834377289\n",
            "Test Error: \n",
            " Accuracy: 85.3%, Avg loss: 0.379699 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.135947 [   32/60000] \n",
            "loss: 0.298530 [ 3232/60000] \n",
            "loss: 0.421750 [ 6432/60000] \n",
            "loss: 0.439180 [ 9632/60000] \n",
            "loss: 0.631683 [12832/60000] \n",
            "loss: 0.524543 [16032/60000] \n",
            "loss: 0.190775 [19232/60000] \n",
            "loss: 0.252007 [22432/60000] \n",
            "loss: 0.268985 [25632/60000] \n",
            "loss: 0.405827 [28832/60000] \n",
            "loss: 0.278177 [32032/60000] \n",
            "loss: 0.413317 [35232/60000] \n",
            "loss: 0.201816 [38432/60000] \n",
            "loss: 0.208332 [41632/60000] \n",
            "loss: 0.531144 [44832/60000] \n",
            "loss: 0.222371 [48032/60000] \n",
            "loss: 0.269393 [51232/60000] \n",
            "loss: 0.262636 [54432/60000] \n",
            "loss: 0.309678 [57632/60000] \n",
            "One epoch takes 284.6871564388275\n",
            "Test Error: \n",
            " Accuracy: 85.9%, Avg loss: 0.383275 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.344737 [   32/60000] \n",
            "loss: 0.398338 [ 3232/60000] \n",
            "loss: 0.324081 [ 6432/60000] \n",
            "loss: 0.352901 [ 9632/60000] \n",
            "loss: 0.231099 [12832/60000] \n",
            "loss: 0.353338 [16032/60000] \n",
            "loss: 0.244953 [19232/60000] \n",
            "loss: 0.136519 [22432/60000] \n",
            "loss: 0.581448 [25632/60000] \n",
            "loss: 0.550015 [28832/60000] \n",
            "loss: 0.102440 [32032/60000] \n",
            "loss: 0.075736 [35232/60000] \n",
            "loss: 0.405320 [38432/60000] \n",
            "loss: 0.192929 [41632/60000] \n",
            "loss: 0.353460 [44832/60000] \n",
            "loss: 0.429932 [48032/60000] \n",
            "loss: 0.372987 [51232/60000] \n",
            "loss: 0.347877 [54432/60000] \n",
            "loss: 0.396789 [57632/60000] \n",
            "One epoch takes 284.0194528102875\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.343836 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.443825 [   32/60000] \n",
            "loss: 0.241940 [ 3232/60000] \n",
            "loss: 0.118254 [ 6432/60000] \n",
            "loss: 0.272015 [ 9632/60000] \n",
            "loss: 0.352613 [12832/60000] \n",
            "loss: 0.407761 [16032/60000] \n",
            "loss: 0.262310 [19232/60000] \n",
            "loss: 0.403963 [22432/60000] \n",
            "loss: 0.219493 [25632/60000] \n",
            "loss: 0.396133 [28832/60000] \n",
            "loss: 0.511693 [32032/60000] \n",
            "loss: 0.466510 [35232/60000] \n",
            "loss: 0.533883 [38432/60000] \n",
            "loss: 0.238048 [41632/60000] \n",
            "loss: 0.320273 [44832/60000] \n",
            "loss: 0.168742 [48032/60000] \n",
            "loss: 0.251148 [51232/60000] \n",
            "loss: 0.264696 [54432/60000] \n",
            "loss: 0.301314 [57632/60000] \n",
            "One epoch takes 283.62681889533997\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.370421 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 0.370421 \n",
            "\n",
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "#@title Training\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  start_time =time.time()\n",
        "  _, _ = train_one_epoch(train_loader, model, loss_fn, optimizer, device=device)\n",
        "  print(f'One epoch takes {time.time() - start_time}')\n",
        "  _, _ = test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "print(\"Training done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHMPOz5QYkNR"
      },
      "source": [
        "**Important**: Run the code below to save the model to your Google Drive, which will need it in the next question (so you do not need to wait for 30min to run it again)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF5OWs_qZERY"
      },
      "outputs": [],
      "source": [
        "#@title Save model to the disk\n",
        "\n",
        "MODEL_PATH = os.path.join(srcpath, 'trained_model_fashionmnist.pth')\n",
        "torch.save(model.state_dict(), MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnUWZRhS-I2k"
      },
      "source": [
        "# Problem B. Save the trained model as ONNX and run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dXcq6RFaH4s"
      },
      "source": [
        "First, load the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTl7h9dsaKMW",
        "outputId": "e8774254-4b69-45fe-cc43-5f2603fbf0ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ViTClassifier()\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3gZAz_pZdfp"
      },
      "source": [
        "In next step, let us define the output path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuccF0WEZdNP"
      },
      "outputs": [],
      "source": [
        "ONNX_PATH = os.path.join(srcpath, 'fashionmnist.onnx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgRUSOTxbhqU"
      },
      "source": [
        "At last, we need to install ONNX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF3OoJgdbjo5",
        "outputId": "bca6244b-32b5-4953-dc57-869443761a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.1 onnxruntime-1.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ9Jkopmbmul"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "import onnxruntime as ort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pXKTeJXaPV_"
      },
      "source": [
        "## Export the model to ONNX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQNsqERYQYE"
      },
      "source": [
        "Given the trained model above, please"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKV3EQVPZbra",
        "outputId": "e17cf08c-970d-4435-94c4-46d4289b4b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2138492928.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model successfully exported to /content/drive/My Drive/cuhk_courses/aims5702/fashionmnist.onnx\n"
          ]
        }
      ],
      "source": [
        "# We first need to define a dummy input. The value does not matter. Just make\n",
        "# sure the tensor shape is correct.\n",
        "dummy_input = torch.zeros((1, 1, 32, 32))\n",
        "\n",
        "# Define names for the input and output nodes\n",
        "input_names = ['input']\n",
        "output_names = ['output']\n",
        "\n",
        "# Define axes that should have dynamic (variable) dimensions\n",
        "# This allows the ONNX model to handle different batch sizes at runtime.\n",
        "dynamic_axes = {\n",
        "    'input': {0: 'batch_size'},   # Make the 0th dimension (batch size) dynamic\n",
        "    'output': {0: 'batch_size'}  # Make the 0th dimension (batch size) dynamic\n",
        "}\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,              # The PyTorch model to export\n",
        "    dummy_input,        # The sample input tensor\n",
        "    ONNX_PATH,          # Where to save the model (file or file-like object)\n",
        "    export_params=True, # Export trained weights with the model\n",
        "    opset_version=17,   # The ONNX version to export to (choose one supported by your target platform)\n",
        "    do_constant_folding=True, # Whether to execute constant folding for optimization\n",
        "    input_names=input_names,  # Names of the model's inputs\n",
        "    output_names=output_names, # Names of the model's outputs\n",
        "    dynamic_axes=dynamic_axes # Specify dynamic dimensions\n",
        ")\n",
        "\n",
        "print(f\"Model successfully exported to {ONNX_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJGHaBhtaqnh"
      },
      "source": [
        "## Problem B.1. Reload the trained model, and test it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboEPlrTa3tE"
      },
      "source": [
        "Let us first reload model and check if it is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_14IfU50a0wL",
        "outputId": "a43add4f-fd01-43f4-9713-bc606f136507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONNX model is well-formed.\n"
          ]
        }
      ],
      "source": [
        "onnx_model = onnx.load(ONNX_PATH)\n",
        "# This raises an exception if the model is invalid\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"ONNX model is well-formed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir1GkaqVWjAd"
      },
      "outputs": [],
      "source": [
        "ort_session = ort.InferenceSession(ONNX_PATH)\n",
        "ort_outputs = ort_session.run(\n",
        "    ['output'], {'input': image.numpy()[np.newaxis, ...]})[0]\n",
        "label = ort_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHAE4DaDa8gl"
      },
      "source": [
        "Then, let us try to run it on testing cases. Please uses test on first 6 samples in testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc8VnVFDZEM7"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3b-1gB2admT"
      },
      "source": [
        "Let us use the first 9 samples as testing cases.\n",
        "\n",
        "To mimic how this may run in actual testing platform, let us remove all transformation.\n",
        "\n",
        "Do not change the block below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-FIfGFeaEzz"
      },
      "outputs": [],
      "source": [
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        ")\n",
        "testing_images = []\n",
        "gt_labels = []\n",
        "for i, (image, label) in enumerate(test_dataset):\n",
        "  testing_images.append(np.asarray(image))\n",
        "  gt_labels.append(class_names[label])\n",
        "  if i == 8:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNYnNCTqa35Y"
      },
      "source": [
        "At last, let us test our model on `testing_images` and compared with ground truth `gt_labels`. And visualize the result.\n",
        "\n",
        "Here we provided code to generate 9 results in testing. And if you not super unlucky, you shall have at least half of them correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "PDUHoQj8a_KY",
        "outputId": "797879d2-6b83-4797-a612-af898f022ff3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x1200 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#####################\n",
        "# Your code goes here\n",
        "#####################\n",
        "ort_session = ort.InferenceSession(ONNX_PATH)\n",
        "\n",
        "est_labels = []\n",
        "eva_images = []\n",
        "eva_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "for i, (image, label) in enumerate(eva_dataset):\n",
        "    eva_images.append(np.asarray(image))\n",
        "    if i == 8:\n",
        "      break\n",
        "for image in eva_images:\n",
        "    ort_outputs = ort_session.run(['output'],\n",
        "     {'input': image.astype(np.float32)[np.newaxis, ...]})[0]\n",
        "\n",
        "    predicted_class_idx = np.argmax(ort_outputs)\n",
        "    est_labels.append(class_names[predicted_class_idx])\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "for i in range(len(testing_images)):\n",
        "  correct = 'CORRECT' if est_labels[i] == gt_labels[i] else 'WRONG'\n",
        "  plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(testing_images[i], cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(f'est={est_labels[i]}, gt={gt_labels[i]}, {correct}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smadw98UeQlS"
      },
      "source": [
        "## Problem B.2 **Bonus**, run ONNX on web page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Pbckm2eVKZ"
      },
      "source": [
        "Please try to build either an HTML that can run the ONNX. This is time consuming and only try it you are comfortable.\n",
        "\n",
        "If you do so, please submit:\n",
        "* A screen record of how you run it.\n",
        "* A zip file of webpage folder.\n",
        "\n",
        "FYI, you can refer to some of these implementations:\n",
        "\n",
        "https://github.com/mxkrn/onnxruntime-web-tutorial/tree/main\n",
        "\n",
        "https://github.com/microsoft/onnxruntime-nextjs-template\n",
        "\n",
        "https://onnxruntime.ai/docs/tutorials/web/classify-images-nextjs-github-template.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y8WdwqAb5eA"
      },
      "source": [
        "# Problem C. Implement multi-head attention by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC1lTG0Fb_VA"
      },
      "source": [
        "In the next step, let us try to implmenet a multihead attention by yourself, and match the implementaiton of pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t89SBQpiU7wB"
      },
      "source": [
        "Here we provided a reference implementation of softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8GtNkeHJVHeA"
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP_mkQAsVMuV"
      },
      "source": [
        "In pytorch, the there are two learnable matrices\n",
        "* Input Projection Matrix (`in_proj_weight`), which consists of weight for K, Q, and V matrices. To reduce your burder, we already chop the input projection into wq, wk, and wq, each of which is `embedding_dim x embedding_dim`.\n",
        "* Output Projection Matrix (`out_proj`), which is another `embedding_dim x embedding_dim`. It takes **combined** multi-head attention result into a single tensor (`batch_size x seq_length x embedding_dim`)\n",
        "Also, to apply this linear matrice to the input feature, it should follow `out_feat[j] = sum_i (in_feat[i] * weight[j, i])`. In another word, the first dim of weight is output feature and second dim is the input feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzGFqoE8XvAl"
      },
      "source": [
        "Also, here is how pytorch deal with multi-head attention\n",
        "* For multi-head attention, it will split the result after input project into `num_head` independent tensors, and combined them on channel dimension after attentions.\n",
        "* For example, if the K tensor before splitting is `1 x 32 x 128`, where batch size is 1 and sequence length is 128. Then, to create 4 heads (`num_head=4`), it will split this matrix into 4 `1 x 32 x 32` K tensors. Q and V are splitted in the same way. After attention, it will obtained 4 `1 x 32 x 32` output tensors. They will be combined into a single `1 x 32 x 128` tensor.\n",
        "* For your reference, we provided `_split_heads` and `_combine_heads` for these two operations, although you can implement your own one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHAKoDgfb74L"
      },
      "source": [
        "At last, to make your life easy, you only need to consider a simplfied cases:\n",
        "* No bias in all calculation\n",
        "* No need to consider attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZAFjnzOYjUgb"
      },
      "outputs": [],
      "source": [
        "#@title CustomizedMultiHeadAttention\n",
        "\n",
        "class CustomizedMultiHeadAttention(object):\n",
        "\n",
        "  def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1):\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dim_each_head = embedding_dim // num_heads\n",
        "\n",
        "  def set_params(self,\n",
        "    param_dict: Dict[str, torch.Tensor]\n",
        "  ) -> None:\n",
        "    \"\"\"\n",
        "    Set the parameters of the layer, like weight and bias of linear layer.\n",
        "\n",
        "    Args:\n",
        "      wq: The weight matrix for Q.\n",
        "      wk: The weight matrix for K.\n",
        "      V_linear: The weight matrix for V.\n",
        "      wout: The weight matrix for the output.\n",
        "    \"\"\"\n",
        "    self.wq = param_dict['wq']\n",
        "    self.wk = param_dict['wk']\n",
        "    self.wv = param_dict['wv']\n",
        "    self.wout = param_dict['wout']\n",
        "\n",
        "  def _split_heads(self, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Splits KQV matrix into multiple heads.\n",
        "\n",
        "    Args:\n",
        "      x: A tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (batch_size, num_heads, sequence_length, dim_each_head)\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    x = x.reshape(batch_size, seq_len,\n",
        "                  self.num_heads, self.dim_each_head).transpose(0, 2, 1, 3)\n",
        "    return x\n",
        "\n",
        "  def _combine_heads(self, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Combines multiple heads into a single matrix.\n",
        "\n",
        "    Args:\n",
        "      x: A tensor of shape\n",
        "          (batch_size, num_heads, sequence_length, dim_each_head)\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "    \"\"\"\n",
        "    batch_size, _, seq_len, _ = x.shape\n",
        "    x = x.transpose((0, 2, 1, 3))\n",
        "    return x.reshape(batch_size, seq_len, self.embedding_dim)\n",
        "\n",
        "  def forward(self, input_k: np.ndarray, input_v: np.ndarray, input_q: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Forward pass of the layer.\n",
        "\n",
        "    Args:\n",
        "      input_k: A tensor of shape (batch_size, kv_length, feature_dim)\n",
        "      input_v: A tensor of shape (batch_size, kv_length, feature_dim)\n",
        "      input_q: A tensor of shape (batch_size, q_length, feature_dim)\n",
        "\n",
        "    Returns:\n",
        "      The output of the layer, shape is (batch_size, sequence_length, embedding_dim).\n",
        "    \"\"\"\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    q_proj = np.matmul(input_q, self.wq.T)\n",
        "    k_proj = np.matmul(input_k, self.wk.T)\n",
        "    v_proj = np.matmul(input_v, self.wv.T)\n",
        "    multi_k = self._split_heads(k_proj)\n",
        "    multi_v = self._split_heads(v_proj)\n",
        "    multi_q = self._split_heads(q_proj)\n",
        "\n",
        "    attention_score = softmax(np.matmul(multi_q, multi_k.transpose(0, 1, 3, 2))/\n",
        "                      np.sqrt(self.dim_each_head))\n",
        "    multi_out = np.matmul(attention_score, multi_v)\n",
        "\n",
        "    combine = self._combine_heads(multi_out)\n",
        "\n",
        "    output = np.matmul(combine, self.wout.T)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8MO54HKf5O6"
      },
      "source": [
        "At last, let us verify the correctness of your impelementation. Please do not change the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yi8qZJbIUdW7"
      },
      "outputs": [],
      "source": [
        "#@title Verification function\n",
        "\n",
        "def verify_output(\n",
        "    actual: Union[np.ndarray, List[np.ndarray]],\n",
        "    reference: Union[np.ndarray, List[np.ndarray]],\n",
        "    name: Union[str, List[str]],\n",
        "    atol: float = 1e-6,\n",
        "    rtol: float = 1e-6\n",
        ") -> None:\n",
        "  \"\"\"\n",
        "  Verify if the actual output is close to the reference output.\n",
        "\n",
        "  Args:\n",
        "    actual: The actual output.\n",
        "    reference: The reference output.\n",
        "    atol: The absolute tolerance.\n",
        "    rtol: The relative tolerance.\n",
        "  \"\"\"\n",
        "  if isinstance(actual, np.ndarray):\n",
        "    close = np.allclose(actual, reference, atol=atol, rtol=rtol)\n",
        "    if not close:\n",
        "      print(f\"{name} is not close to the reference output.\")\n",
        "    else:\n",
        "      print(f\"{name} is close to the reference output.\")\n",
        "  elif isinstance(actual, List):\n",
        "    close = all(verify_output(a, b, name[i], atol=atol, rtol=rtol)\n",
        "                for i, (a, b) in enumerate(zip(actual, reference)))\n",
        "  return close\n",
        "\n",
        "def compare_with_actual_layer(\n",
        "    official_multi_head_attention: nn.modules,\n",
        "    customized_multi_head_attention: CustomizedMultiHeadAttention,\n",
        "    input_k: np.ndarray,\n",
        "    input_v: np.ndarray,\n",
        "    input_q: np.ndarray,\n",
        "    wq: np.ndarray,\n",
        "    wk: np.ndarray,\n",
        "    wv: np.ndarray,\n",
        "    wout: np.ndarray,\n",
        "    a_tol: float = 1e-6,\n",
        "    r_tol: float = 1e-6,\n",
        ") -> bool:\n",
        "  \"\"\"\n",
        "  Compare forward of your customized layer with the official layer.\n",
        "\n",
        "  Args:\n",
        "    official_layer: The official layer.\n",
        "    customized_layer: The customized layer.\n",
        "    input_k: The input numpy array for K (batch_size, kv_length, feature_dim).\n",
        "    input_v: The input numpy array for V (batch_size, kv_length, feature_dim).\n",
        "    input_q: The input numpy array for Q (batch_size, q_length, feature_dim).\n",
        "    wq: The input numpy array for WQ (feature_dim, feature_dim).\n",
        "    wk: The input numpy array for WK (feature_dim, feature_dim).\n",
        "    wv: The input numpy array for WV (feature_dim, feature_dim).\n",
        "    wout: The input numpy array for WOUT (feature_dim, feature_dim).\n",
        "    a_tol: The absolute tolerance.\n",
        "    r_tol: The relative tolerance.\n",
        "\n",
        "  Returns:\n",
        "    True if the output of the customized layer is close to the output of\n",
        "    the official layer, False otherwise.\n",
        "  \"\"\"\n",
        "  success = True\n",
        "  input_k_torch, input_v_torch, input_q_torch = (\n",
        "      torch.tensor(input_k),\n",
        "      torch.tensor(input_v),\n",
        "      torch.tensor(input_q)\n",
        "  )\n",
        "\n",
        "  # Set parameters\n",
        "  official_multi_head_attention.in_proj_weight.data.copy_(\n",
        "    torch.tensor(np.concatenate([wq, wk, wv], axis=0))\n",
        "  )\n",
        "  official_multi_head_attention.out_proj.weight.data.copy_(\n",
        "    torch.tensor(wout)\n",
        "  )\n",
        "  customized_multi_head_attention.set_params({\n",
        "    'wq': wq,\n",
        "    'wk': wk,\n",
        "    'wv': wv,\n",
        "    'wout': wout\n",
        "  })\n",
        "\n",
        "  # Run forward.\n",
        "  output_customized_np = customized_multi_head_attention.forward(\n",
        "    input_k, input_v, input_q\n",
        "  )\n",
        "  output_official_torch, _ = official_multi_head_attention(\n",
        "      input_q_torch, input_k_torch, input_v_torch)\n",
        "  output_official_np = output_official_torch.detach().numpy()\n",
        "\n",
        "  # Check output\n",
        "  success = verify_output(output_customized_np, output_official_np,\n",
        "                          'Output', atol=a_tol, rtol=r_tol)\n",
        "  if not success:\n",
        "    return success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp1ipbIOf4b2",
        "outputId": "680a02fc-0c9a-4e74-cdde-3955e12c816f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Testing with batch_size = 1 feature_dim = 16 num_heads = 4 ===\n",
            "Output is close to the reference output.\n",
            "=== Testing with batch_size = 1 feature_dim = 128 num_heads = 8 ===\n",
            "Output is close to the reference output.\n",
            "=== Testing with batch_size = 1 feature_dim = 16 num_heads = 4 ===\n",
            "Output is close to the reference output.\n",
            "=== Testing with batch_size = 8 feature_dim = 16 num_heads = 4 ===\n",
            "Output is close to the reference output.\n"
          ]
        }
      ],
      "source": [
        "#@title Run verification\n",
        "\n",
        "for batch_size, feature_dim, num_heads, sequence_length in [\n",
        "    (1, 16, 4, 7),\n",
        "    (1, 128, 8, 7),\n",
        "    (1, 16, 4, 243),\n",
        "    (8, 16, 4, 7),\n",
        "]:\n",
        "  print(f'=== Testing with batch_size = {batch_size} '\n",
        "        f'feature_dim = {feature_dim} '\n",
        "        f'num_heads = {num_heads} ===')\n",
        "  def _apply_norm(x):\n",
        "    with torch.no_grad():\n",
        "      norm = torch.nn.LayerNorm(feature_dim)\n",
        "      x_torch = torch.tensor(x)\n",
        "      normed_torch = norm(x_torch)\n",
        "    return normed_torch.numpy()\n",
        "  # Mimic layered norm when generating input\n",
        "  input_k = _apply_norm(np.random.randn(\n",
        "      batch_size, sequence_length, feature_dim).astype(np.float32))\n",
        "  input_v = _apply_norm(np.random.randn(\n",
        "      batch_size, sequence_length, feature_dim).astype(np.float32))\n",
        "  input_q = _apply_norm(np.random.randn(\n",
        "      batch_size, sequence_length, feature_dim).astype(np.float32))\n",
        "  wq = np.random.randn(feature_dim, feature_dim).astype(np.float32)\n",
        "  wk = np.random.randn(feature_dim, feature_dim).astype(np.float32)\n",
        "  wv = np.random.randn(feature_dim, feature_dim).astype(np.float32)\n",
        "  wout = np.random.randn(feature_dim, feature_dim).astype(np.float32)\n",
        "\n",
        "  customized_attention = CustomizedMultiHeadAttention(\n",
        "      feature_dim, num_heads)\n",
        "  torch_attention = nn.MultiheadAttention(\n",
        "      feature_dim, num_heads, batch_first=True, bias=False)\n",
        "\n",
        "  compare_with_actual_layer(\n",
        "      torch_attention,\n",
        "      customized_attention,\n",
        "      input_k, input_v, input_q, wq, wk, wv, wout,\n",
        "      a_tol=1e-4, r_tol=1e-4,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv_65JxNxM3L"
      },
      "source": [
        "# Problem D. Transformer on Text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpKi3zsf8cp3"
      },
      "source": [
        "First, for this question, we need torchtext library.\n",
        "\n",
        "Unfortunately, due to some version issues, you need to reinstall torch.\n",
        "\n",
        "So please **start a fresh new session** and run the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrfhpJgInRQ8",
        "outputId": "99804879-87d8-49a6-86be-18ef6e6e55f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision-0.23.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libcudart.45e7f3ed.so.12\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libjpeg.bd6b9199.so.8\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libnvjpeg.e5f20359.so.12\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libpng16.0481ee11.so.16\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libsharpyuv.b609dd4c.so.0\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libwebp.58a855fe.so.7\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libz.622bbd06.so.1\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision/*\n",
            "Proceed (Y/n)?   Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/torchaudio-2.8.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torio/*\n",
            "Proceed (Y/n)?   Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.12/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torch-2.8.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchgen/*\n",
            "Proceed (Y/n)?   Successfully uninstalled torch-2.8.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121\n",
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext==0.18.0) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n",
            "Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n",
            "Collecting torchdata==0.9.0\n",
            "  Downloading torchdata-0.9.0-cp312-cp312-manylinux1_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.9.0) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata==0.9.0) (2.32.4)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.9.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.9.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.9.0) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.9.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.9.0) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.9.0) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchdata==0.9.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2->torchdata==0.9.0) (1.3.0)\n",
            "Downloading torchdata-0.9.0-cp312-cp312-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchdata-0.9.0\n",
            "Collecting portalocker==2.8.2\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!yes | pip uninstall torchvision torchaudio torch\n",
        "!yes | pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!yes | pip install torchtext==0.18.0\n",
        "!yes | pip install torchdata==0.9.0\n",
        "!yes | pip install portalocker==2.8.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lXOy-xIa7JA"
      },
      "source": [
        "This is an old version of code, so you will see some degradation warning. Just ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlGVSPwQxOu1",
        "outputId": "58b82436-591b-4323-ba93-58c7c17f0c80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "import torchtext.datasets as datasets\n",
        "import torchtext.vocab as torchvocab\n",
        "\n",
        "from typing import List, Tuple, Iterable\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKNzIgxuXOWK"
      },
      "source": [
        "First, define some constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJZQowiOXOEs"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 256  # Maximum token length\n",
        "batch_size = 32\n",
        "vocab_cap = 30000  # Size of vocabulatory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkpMpu5-BvMn"
      },
      "source": [
        "## Problem description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo6iXCDqBxch"
      },
      "source": [
        "The IMDB dataset contains several movie comments, and one label, whether this comment is positive or negative. We will train a transformer to check to predict this label (positive or negative) from the input comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gsY88bjByIW"
      },
      "source": [
        "To achieve this, the main different from Problem A is how to encode text. Here we use a simple encoder, which basically does follows:\n",
        "\n",
        "* **Raw input**, which will be a sentence, like `This is a terrible movie`\n",
        "* **Chop into tokens**. Here we chop it into a single word (token) and convert to lower cases, like `this` `is` `a` `terrible` `movie`. This is done through a simple tokenizer (5 tokens).\n",
        "* **Convert tokens to ID**. We further convert each token (letter) into a ID (a number), so that it can be process by the network. This is done through a simple vocabulary (dictionary).\n",
        "* **Compute the embedding feature of ID**. Since a single scalar is hard to be process by the network, we further encode them into a vector, using `nn.Embedding`.\n",
        "\n",
        "The chop_into_token and convert_token_to_Id steps are done in the dataset creation, and embedding feature calculation is done in the first step of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhmo4ZqFLPwP"
      },
      "source": [
        "## Problem D.1 Build dataset (just run it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggKgwFvzdKmR"
      },
      "source": [
        "Let us first load the raw text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndls1pscdM-q",
        "outputId": "9e158491-58e9-41ee-bc50-2cd8aa7d032d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training data is 25000\n",
            "Number of testing data is 25000\n"
          ]
        }
      ],
      "source": [
        "raw_train_data = list(datasets.IMDB(split='train'))\n",
        "raw_test_data = list(datasets.IMDB(split='test'))\n",
        "\n",
        "print(f'Number of training data is {len(raw_train_data)}')\n",
        "print(f'Number of testing data is {len(raw_test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY4wcQBxdiua"
      },
      "source": [
        "And let us visualize how the data may look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVyZ9qQidl1m",
        "outputId": "0aa9dd1b-cfd5-4522-81a7-64c5edbbeeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== These are negative feedbacks ====\n",
            "(1, 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')\n",
            "(1, \"Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.<br /><br />OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\")\n",
            "(1, 'This is an action Western. James Steart leads an all star cast in the scenic Northwest, which is filmed in great splendor. The scenery and costumes are great. There is action and adventure. Stewart plays a wealthy cattleman who runs afoul of a crooked government in the old Nothwest.<br /><br />The main drawback is the stereotypical cynic that Hollywood has always made into a hero. Even when this movie was made, the cynic was the stereotypical hero, and the one Stewart portrays really has few saving graces. He is kind to his two partners, and that does give him an extra dimension of credibility and likability.<br /><br />However, he is so piggish to everyone else, it is hard to really care for him, or to accept him. He is much like the one dimensional spaghetti Western characters (cut not that bad).<br /><br />Still, the minor characters are quite enjoyable. Walter Brennan, Royal Dano, Harry Morgan, and others make this worth watching.')\n",
            "(1, 'Unlike \"The Adventures of Buckaroo Banzai\", or \"Big Trouble in Little China\", or \"Conan the Barbarian\", which are horrible films that have a certain coolness and self-deprecating humor that turn them into cult sensations, The Golden Child is just plain bad.<br /><br />The premise itself is not unworkable, and there are some funny moments. But here the Eddy Murphy \"flip attitude\" just deflates any feeling of tension or danger in the story. And the special effects are silly enough to do more damage to that tension. The \"mystic secrets\" of Tibetan Buddhism are lampooned rather than drawn upon to compel.<br /><br />Without a feeling that anything is at stake, or that the characters are faced by real danger, why should we care?<br /><br />Who should see this film:<br /><br />-- big fans of Eddy Murphy who can\\'t help themselves<br /><br />-- I can\\'t think of anyone else<br /><br />I\\'ll give this film a 4 out of 10 for the occasional joke that worked.')\n",
            "(1, \"This was an incredibly stupid movie. It was possibly the worst movie I've ever had the displeasure of sitting through. I cannot fathom how it ranks a rating of 5 or 6.............\")\n",
            "==== These are positive feedbacks ====\n",
            "(2, 'Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn\\'t really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I\\'d have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.')\n",
            "(2, 'Emilio Miraglio\\'s \"The Red Queen Kills Seven Times\" (1972) is just about the most perfect example of a giallo that I have ever seen, mixing all the requisite elements into one sinister stew indeed. First of all, and of paramount importance for me, it has a complex, twisty plot that ultimately makes perfect sense, and the killer here does not come completely out of left field at the end. The story, concerning a series of gruesome murders (you already know how many from the film\\'s title, right?) that takes place in seeming fulfillment of an ancient prophecy concerning two sisters, is an involving one, and the murderer, a red-cloaked figure with the insane laugh of a madwoman, is both frightening and memorable. Every great giallo requires some lovely lead actresses, and here we have quite an assortment, headed by the ridiculously beautiful Barbara Bouchet as one of the two sisters and, in one of her earlier roles, Sybil Danning, as a lustful tramp at Barbara\\'s fashion house. Another necessary ingredient of a superior giallo is a catchy, hummable score, and Bruno Nicolai provides one for this film that should stay with you for days. Gorgeous scenery? Check again. Filmed largely in Wurzburg, Germany, the picture is a treat for the eye indeed. OK, OK, but what about those murders? After all, isn\\'t that what gialli are all about? Well, I\\'m pleased to report that most viewers should be well satisfied with the various knifings, shootings, impalements and other carnage that this film tastefully dishes out...not to mention the crypts, freaky dream sequence, rats and bats (and LOTS of \\'em, too!), the drug references, a rape scene, the obligatory red herrings and, in the person of Ugo Pagliai, a hunky leading man for the female viewers. As I said, a perfect giallo. And even better, this DVD is from the fine folks at No Shame, and you know what that means: a gorgeous print and loads of extras, to boot! Thanks, guys!')\n",
            "(2, 'Exquisite comedy starring Marian Davies (with the affable William Haines). Young Peggy arrives in Hollywood seeking stardom. Cameo performances showcase \"all the stars in MGM\\'s heaven\" in the famous commissary scene, plus lots of vintage film making detail for the scholar. Pic also captures for posterity Davies\\' famous, wickedly sarcastic impersonations of the top stars of the day (her Swanson is a beaut!).<br /><br />\"Peggy,\" even catches herself as she encounters the famous star Marian Davies at tennis, turns up her nose and comments, \"Ohh, I don\\'t like her!\"<br /><br />My print was perfect. Story, direction, acting an authentic charm and a must for all silent afficinados.')\n",
            "(2, 'I read Schneebaum\\'s book (same title as this film) when it was first published and was deeply moved by his ability to see through the many ways of \"otherness\" (his own and the people of the Amazon with whom he lived and loved) to a way of living a decent life. His subsequent books were not as powerful, but showed his continuing quest. His description of his sexual relations with the men of the tribe was way ahead of its time in the early 60\\'s, but his honesty and openness about it were welcome. This movie beautifully conveys both the quirkiness and generosity of the man, but also provides a glimpse into the inevitable destruction of innocence (which is not a morally positive term, in this case) that occurs when \"civilized\" men intrude on traditional societies. Even so, Schneebaum himself has moved into a kind of higher innocence that suggests the possibility of saving humanity from its own destructiveness.')\n",
            "(2, \"<br /><br />There is STAR TREK canon -- lots of it. From canon we know the history of the future. Advances in technology, events, places, first contacts with new beings, names, dates, etc.<br /><br />ENTERPRISE pretty much disregards much of ST canon. An unfortunate fact for long time serious fans. As one, I assumed that the producers would at least take a look at the first few episodes of TOS and retro back from there -- but no.<br /><br />The phase pistols, like much of the technology, look much more modern than found in TOS. An old style Starfleet laser gun, a slow gold speckle transporter effect -- that's what I expected to see. Also, I did not expect to hear pure beep-based sound effects similar to TNG but far apart from TOS sound effects. <br /><br />In the earliest view of TOS (the original pilot: THE CAGE), we see a Starfleet with a more formal military aspect -- a bit of old earth Navy. With ENTERPRISE, we see a shocking disregard for rank. There is more military code in the cartoon STAR BLAZERS than in ENTERPRISE.<br /><br />It is fine that Captain Archer is unsure about the needs of the Universe (quite unlike Kirk who never lacked confidence in his application of human justice), but inside ENTERPRISE everyone seems like an equal. Unprofessional, unsure, more distant from the feel of formal military service than found in any ST series -- and that says a lot!<br /><br />The casual country music opening theme song heralds the journeys of a family rather than the adventures of an important large military vessel.<br /><br />ENTERPRISE looks to show us a mostly fun, warm-fuzzy exploration of human relationships rather than take us on a historic, bold, gritty, high-rick exploration of space.<br /><br />I would have selected Adrian Paul to play the Captain and an older human to be the doctor. Still, I liked the actors for the most part. Linda Park, an outstanding ballroom dancer from Boston College, is sure to develop nicely. The characters making up the crew seem to be thoughtfully created.<br /><br />ENTERPRISE begins its run stronger than did the past three STAR TREK series. Let's hope for a good future!\")\n"
          ]
        }
      ],
      "source": [
        "for i, name in [(1, 'negative'), (2, 'positive')]:\n",
        "  examples = [x for x in raw_train_data if x[0] == i]\n",
        "  print(f'==== These are {name} feedbacks ====')\n",
        "  for example in examples[::100][:5]:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpDojzmGIHZB"
      },
      "source": [
        "First, let us define a simple tokenizer. Just chop the sentence into words and convert them to lower cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oljhOcwZINxp"
      },
      "outputs": [],
      "source": [
        "#@title A simple tokenizer\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "  \"\"\"For generalization, we just convert all letters to lower cases.\"\"\"\n",
        "  return re.findall(r\"[A-Za-z0-9']+\", text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNMXoalRK9bM"
      },
      "source": [
        "And let us try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGeS5jp8K82C",
        "outputId": "f904b2f8-56b1-4be7-921d-a3d598e26303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'bad', 'movie']\n"
          ]
        }
      ],
      "source": [
        "raw_str = 'This is a bad movie'\n",
        "print(simple_tokenize(raw_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FhPkDC0LvTD"
      },
      "source": [
        "Here we also need to define some special tokens.\n",
        "* PAD is used to pad the sentence when the input sentences is shorter than `max_seq_length`\n",
        "* UNK is used when vocabulatory cannot encode a new token.\n",
        "* CLS is the CLS token as Problem A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3ijfB9HLzLX"
      },
      "outputs": [],
      "source": [
        "PAD, UNK, CLS = '<pad>', '<UNK>', '<cls>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcuwmtqUHkSX"
      },
      "source": [
        "Then, we can calculate the vocabulary, which we will use it to convert each word into ID. Here we use function from torchtext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfsA9wqCHkFP"
      },
      "outputs": [],
      "source": [
        "#@title Create vocabulary\n",
        "\n",
        "def create_vocab(\n",
        "    raw_data: List[Tuple[int, str]],\n",
        "    vocab_cap: int = 30000\n",
        ") -> torchvocab.Vocab:\n",
        "  \"\"\"\n",
        "  Create a vocabulary from the raw training data.\n",
        "\n",
        "  Args:\n",
        "    raw_data: The raw training data.\n",
        "    vocab_cap: The maximum number of tokens in the vocabulary.\n",
        "\n",
        "  Returns:\n",
        "    A vocabulary.\n",
        "  \"\"\"\n",
        "  def _yield_tokens(examples: Iterable[Tuple[str, str]]):\n",
        "    # torchtext IMDB: (label, text)\n",
        "    for label, text in examples:\n",
        "      yield simple_tokenize(text)\n",
        "  simple_vocab = torchvocab.build_vocab_from_iterator(\n",
        "      _yield_tokens(raw_data), specials=[PAD, UNK, CLS],\n",
        "      max_tokens=vocab_cap)\n",
        "  simple_vocab.set_default_index(simple_vocab[UNK])\n",
        "  return simple_vocab\n",
        "\n",
        "simple_vocab = create_vocab(raw_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-G86YULVY0"
      },
      "source": [
        "Let us try it with the simple tokenizer. Note the last token is unknown, so vocab output UNK (ID=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByctHFB_Ib8U",
        "outputId": "111e80d4-f1a5-4b40-e89a-ad1f860188aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "special token `<pad>` is encoded as 0\n",
            "special token `<UNK>` is encoded as 1\n",
            "special token `<cls>` is encoded as 2\n",
            "word `this` is encoded as 13\n",
            "word `is` is encoded as 8\n",
            "word `a` is encoded as 5\n",
            "word `bad` is encoded as 77\n",
            "word `movie` is encoded as 19\n",
            "word `dafhancbahgdh` is encoded as 1\n"
          ]
        }
      ],
      "source": [
        "for word in [PAD, UNK, CLS]:\n",
        "  print(f'special token `{word}` is encoded as {simple_vocab[word]}')\n",
        "\n",
        "raw_str = 'This is a bad movie dafhancbahgdh'\n",
        "for word in simple_tokenize(raw_str):\n",
        "  print(f'word `{word}` is encoded as {simple_vocab[word]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZddwyMRUexHM"
      },
      "source": [
        "Then let us create a function that converts raw text into IDs, using the vocabulatary above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8TdNOd_hDVn"
      },
      "outputs": [],
      "source": [
        "#@title Create dataset preprocessor.\n",
        "\n",
        "class PadCollate:\n",
        "  def __init__(self, max_len: int, pad_id: int, cls_id: int, vocab: any):\n",
        "    self.max_len = max_len\n",
        "    self.pad_id = pad_id\n",
        "    self.cls_id = cls_id\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: A list of tuples, each containing a list of tokens\n",
        "            (tokenized movie comments) and a label (1 or 2).\n",
        "\n",
        "    Returns:\n",
        "      A tuple containing the input ids, the key padding mask, and the labels.\n",
        "    \"\"\"\n",
        "    # batch: List[(tokens, label)]\n",
        "    ids_list, labels, lengths = [], [], []\n",
        "    for toks, label in batch:\n",
        "      ids = [self.vocab[token] for token in toks][: self.max_len - 1]\n",
        "      ids = [self.cls_id] + ids\n",
        "      ids_list.append(ids)\n",
        "      labels.append(label)\n",
        "      lengths.append(len(ids))\n",
        "\n",
        "    T = min(self.max_len, max(lengths))\n",
        "    B = len(batch)\n",
        "\n",
        "    input_ids = torch.full((B, T), self.pad_id, dtype=torch.long)\n",
        "    for i, ids in enumerate(ids_list):\n",
        "      cur = ids[:T]\n",
        "      input_ids[i, :len(cur)] = torch.tensor(cur, dtype=torch.long)\n",
        "\n",
        "    # key_padding_mask: True is for padded token\n",
        "    # (will be ignored by the Transformer encoder)\n",
        "    # Shape is [batch_size, sequence_length], bool\n",
        "    key_padding_mask = (input_ids == self.pad_id)\n",
        "\n",
        "    # Also, packed the labels\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return input_ids, key_padding_mask, labels\n",
        "\n",
        "pad_collate = PadCollate(\n",
        "    max_len=256,\n",
        "    pad_id=simple_vocab[PAD],\n",
        "    cls_id=simple_vocab[CLS],\n",
        "    vocab=simple_vocab\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLEK4GNpSKEr",
        "outputId": "3f04fdf7-fdab-443f-9db6-f43d8277e58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ID is a matrix with shape [sample_size x max_length]\n",
            "For example, for the first 100 samples, the size of ids is 100x256\n"
          ]
        }
      ],
      "source": [
        "ids, key_mask, _ = pad_collate([(y, x) for x, y in raw_train_data[:100]])\n",
        "\n",
        "print('The ID is a matrix with shape [sample_size x max_length]')\n",
        "print('For example, for the first 100 samples, the size of ids is '\n",
        "      f'{ids.shape[0]}x{ids.shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz8jCgXJSfS4"
      },
      "source": [
        "Note that some sentences are short, which cannot reach the maximum token length. For those sentences, we use key_padding_mask, like this one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-3qKKZCSrWO",
        "outputId": "6c910994-6a23-4856-c895-6007e50c5ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== This is the raw input text ===\n",
            "Ned aKelly is such an important story to Australians but this movie is awful. It's an Australian story yet it seems like it was set in America. Also Ned was an Australian yet he has an Irish accent...it is the worst film I have seen in a long time\n",
            "The length of this sentence is 247, which is smaller than the 256.\n",
            "\n",
            "=== And this is encoded results. ===\n",
            "Note that the last few tokens are 0, which shows these are padded token.\n",
            "tensor([   2,    1,  796,  818,    1,    5,    1,  796, 1718, 1718, 4896,    1,\n",
            "          12,  398,    1,  398, 1208, 1074, 1969,    1,    5, 2607,    1,   12,\n",
            "        1696, 1562, 1452, 1367,  746,    5, 2607,  746,    1,  398,  746, 1452,\n",
            "        1367, 4896,    1,  746, 1452,    1,    1, 1208,  398,  746, 1367,    5,\n",
            "        1718,   12,    5, 2607,  398,    1,  496, 1208,  746,    1,  746, 1969,\n",
            "          12,  398,    1, 1696, 1452, 1895,   12,  796,    1,   12,  398,    1,\n",
            "           5, 1993, 1167, 1208, 1718,    1,    1,    1,  746,  749,  398,    1,\n",
            "           5, 2607,    1,    1, 1208,  398,  746, 1367,    5, 1718,   12,    5,\n",
            "        2607,    1,  398,  746, 1452, 1367, 4896,    1, 4896,  796,  746,    1,\n",
            "          12,  746,    1,  398,  796,  796, 1696,  398,    1, 1718,   12, 2232,\n",
            "         796,    1,   12,  746,    1, 1993,    5,  398,    1,  398,  796,  746,\n",
            "           1,   12, 2607,    1,    1, 1696,  796, 1367,   12, 1074,    5,    1,\n",
            "           1,    1, 1718,  398, 1452,    1,    1,  796,  818,    1, 1993,    5,\n",
            "         398,    1,    5, 2607,    1,    1, 1208,  398,  746, 1367,    5, 1718,\n",
            "          12,    5, 2607,    1, 4896,  796,  746,    1, 1969,  796,    1, 1969,\n",
            "           5,  398,    1,    5, 2607,    1,    1, 1367,   12,  398, 1969,    1,\n",
            "           5, 1074, 1074,  796, 2607,  746,    1,    1,    1,   12,  746,    1,\n",
            "          12,  398,    1,  746, 1969,  796,    1, 1993, 1452, 1367,  398,  746,\n",
            "           1, 1167,   12, 1718, 1696,    1,    1,    1, 1969,    5, 1895,  796,\n",
            "           1,  398,  796,  796, 2607,    1,   12, 2607,    1,    5,    1, 1718,\n",
            "        1452, 2607, 1226,    1,  746,   12, 1696,  796,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "\n",
            "=== This is masked out in the key masking. So we will not calculate them. ===\n",
            "tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True])\n"
          ]
        }
      ],
      "source": [
        "index = 59\n",
        "sample_text = raw_train_data[index][1]\n",
        "sample_label = raw_train_data[index][0]\n",
        "\n",
        "print('=== This is the raw input text ===')\n",
        "print(raw_train_data[index][1])\n",
        "print(f'The length of this sentence is {len(sample_text)}, '\n",
        "      f'which is smaller than the {max_seq_length}.')\n",
        "print('\\n=== And this is encoded results. ===')\n",
        "print('Note that the last few tokens are 0, '\n",
        "      'which shows these are padded token.')\n",
        "print(ids[index, :])\n",
        "print('\\n=== This is masked out in the key masking. '\n",
        "      'So we will not calculate them. ===')\n",
        "print(key_mask[index, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3xNVflNXAGr"
      },
      "source": [
        "Finally, we can build the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsVdzw1zl1qD"
      },
      "outputs": [],
      "source": [
        "#@title Finally create the dataset\n",
        "\n",
        "class IMDBListDataset(Dataset):\n",
        "  def __init__(self, examples: List[Tuple[str, str]]):\n",
        "    self.data = []\n",
        "    for raw_label, text in examples:\n",
        "      label = 1 if raw_label == 2 else 0\n",
        "      toks = simple_tokenize(text)\n",
        "      self.data.append((toks, label))\n",
        "  def __len__(self): return len(self.data)\n",
        "  def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "train_loader = DataLoader(IMDBListDataset(raw_train_data), batch_size=batch_size,\n",
        "                          shuffle=True, collate_fn=pad_collate, num_workers=2)\n",
        "test_loader  = DataLoader(IMDBListDataset(raw_test_data), batch_size=batch_size,\n",
        "                          shuffle=False, collate_fn=pad_collate, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Nn49reBRpb"
      },
      "source": [
        "Visualize the first 2 samples in the `train_loader`. Note the loader output is different from previous Problem A and when running the training iteration, we need to change `train_one_epoch` function accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDrUNrVcEu0G",
        "outputId": "7914eb01-cbe7-401a-bfef-a70578d5b2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of input_ids is torch.Size([32, 256])\n",
            "The shape of mask is torch.Size([32, 256])\n",
            "The shape of label is torch.Size([32])\n",
            "==== Sample 0 ====\n",
            "input_ids tensor([    2,    12,   118,   922,   926,     4,   243,  1689,     7,   275,\n",
            "           55,    18,     5,   175,   922,   926,   101,   247,    25,   631,\n",
            "            1,   468,  2522,     4,  7227,    12,   805,     7,    39,    13,\n",
            "           12,    65,   121,    20,    11,     8,     7,    51,   247,   922,\n",
            "          926,    16, 14951,   456,     8,     7,   322,  2142,     3,   203,\n",
            "          708, 19190,   705,  3440,   728,  4582,  3959,  4424,    14,   151,\n",
            "         1016,     3,   976,     4,  2147,    30,  2041,   104,   564,    29,\n",
            "         3080,    18,     5,     1,     1,   958,   145,   251,    49,    25,\n",
            "          147,     6,    24,    45,    49,    36,   103, 14951,   456,     8,\n",
            "           51,   922,   926,   247,    44,    23,    44,   741,   818,     4,\n",
            "         8773,   136,   177,   799,   237,    39,  1423,     4,   108,   944,\n",
            "          922,   926,     8,     5,   512,    14,   126,    23,   192,   410,\n",
            "          611, 20805,   322,  2142,    11,   202,  1693,   672,  1340,   245,\n",
            "           23,    16,     5,   620,  4037,    44,    65,   880,     7,   459,\n",
            "           43,     3,   104,   132,    16,    35,    25,    23,   330,  6389,\n",
            "           42,  1014,     5,  5398,     6,   112,    67,  1727,     4,  3378,\n",
            "           25,  1644,     4,   726,   400,  1354,     7,   105,     3,  1189,\n",
            "            6,   700,   123,    44,  1917,    16,    35,    27,     7,   209,\n",
            "          134,  1924,     1,   700,   900,    83,    61,    23,  1746,   148,\n",
            "            1, 11616,   214,    29,  1589,    10,    67, 15890,    16,    13,\n",
            "          755,   705,   862,  1985,   148,    11,   208, 16135,  2009,    65,\n",
            "          961,    13,   343,     1,     1,     6,     5,   122,     1,    82,\n",
            "          835,  1362,    37,   515,   124,     5,   292,   108,     4,    94,\n",
            "          719,    88,   144,    16,   158,   283,     1,  7725,    31,   119,\n",
            "          173,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "mask tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True])\n",
            "label is tensor(0)\n",
            "==== Sample 1 ====\n",
            "input_ids tensor([    2,   289,     3,   720,  1107,     6,     5,  2361,   261,    46,\n",
            "           24,    39,   205,   101,    13,    30,   945,     3,   643,   518,\n",
            "         3220,  1856,    18,     3,    87,  1172,  5477,  1970,   405,  1205,\n",
            "         3356,    18,     3,  1673,  6177,  3806,  7359,     4,    59,  4286,\n",
            "          405,  5854,    31,     6,    13,     8,   441,     4,  9702,  2394,\n",
            "           20,    49,     8,   163,    14,    65,  3632,    24,   244,    46,\n",
            "          876,   109,   128,  1499,   159,     9,     9,     3,   113,     8,\n",
            "           92,   220,    33,     3,  7138,     6,     5,  4788,    62,     8,\n",
            "         1096,    20,   983,  3327,   110,     6,     3,   104,    25,  2701,\n",
            "         7570,     3,  1612,  7475,     3,  5666,  4654,  2526,     3,  8645,\n",
            "         1787,     3,  7031, 19877,     3,  8448,  1210,   761,  1713,   419,\n",
            "            3,  8645,  5896,     3,  1922,     1,    36,   271,    39,    28,\n",
            "           15, 12253,    16,     3,  2662,    10,     5,   362,    19,    38,\n",
            "            3,  6259,    31,     1,   918,    20,   173,   163,   317,     9,\n",
            "            9,    12,   196,     3,   292,  1909,    70,   183,    72,   223,\n",
            "            4,  1018,    72,   918,    33,     3,   129,     6,     3,    19,\n",
            "           24,   434,   694,    36,     3,    51,   492,    70,     4,  1173,\n",
            "           49,    15,    34,   921,  5817,    16,     3,    65,    77,   658,\n",
            "          187,    67,    42, 12366,    54,  4282,    20,    94,    24,  1173,\n",
            "         1017,  3426,   207,     3,    63,   154,    12,   257,    65,   614,\n",
            "           15,     3,  1813,  1903,     7,     1,   578,   314,     3,   235,\n",
            "          547,   135,     9,     9,    23,    77,    23,    51,  5108,   465,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "mask tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True])\n",
            "label is tensor(0)\n"
          ]
        }
      ],
      "source": [
        "for batch_index, (ids, key_masking, labels) in enumerate(test_loader):\n",
        "  print(f'The shape of input_ids is {ids.shape}')\n",
        "  print(f'The shape of mask is {key_masking.shape}')\n",
        "  print(f'The shape of label is {labels.shape}')\n",
        "  for i in range(2):\n",
        "    print(f'==== Sample {i} ====')\n",
        "    print('input_ids', ids[i, :])\n",
        "    print('mask', key_masking[i, :])\n",
        "    print('label is', labels[i])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VGeDvUmXXdZ"
      },
      "source": [
        "## Problem D.2. Model creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUvbBmj9Xc9V"
      },
      "source": [
        "Now, let us define the network. This is very similar to problem A. First, you can copy your implementation of PositionEncoding here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqwsb-Fp_IME"
      },
      "outputs": [],
      "source": [
        "def power_base_10000(x):\n",
        "  \"\"\"\n",
        "  Computes the power of 10000.0 to the input tensor.\n",
        "\n",
        "  Args:\n",
        "    x: The input tensor.\n",
        "  \"\"\"\n",
        "  return torch.exp(math.log(10000.0) * x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  \"\"\"\n",
        "  Adds positional information to the patch embeddings.\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, max_seq_length=512):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      embedding_dim: The dimension of the embedding.\n",
        "      max_seq_length: The maximum sequence length.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    position_encoding_table = torch.zeros(max_seq_length, embedding_dim)\n",
        "    # Create a position table.\n",
        "    # position_encoding_table[i, :] should be length embedding_dim vector that\n",
        "    # encodes the position i.\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    w_k = power_base_10000(torch.arange(0, embedding_dim, 2)/embedding_dim)\n",
        "    position_encoding_table[:, 0::2] = torch.sin(position * w_k)\n",
        "    position_encoding_table[:, 1::2] = torch.cos(position * w_k)\n",
        "    self.register_buffer('position_encoding_table', position_encoding_table)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: The input tensor with shape\n",
        "          (Batch_Size, Sequence_Length, Embedding_Dim).\n",
        "\n",
        "    Returns:\n",
        "      The output tensor with shape (Batch_Size, Sequence_Length, Embedding_Dim).\n",
        "    \"\"\"\n",
        "    _, seq_length, _ = x.shape\n",
        "    return x + self.position_encoding_table[np.newaxis, :seq_length, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwSZMnv8Y0ZF"
      },
      "source": [
        "At last, create the Transformer.\n",
        "\n",
        "Hints:\n",
        "* Check Problem A for how to define the network\n",
        "* To apply key_padding_masking in attention, you can call it like: `output_tokens = transformer_encoder(input_tokens, src_key_padding_mask=key_padding_mask)`\n",
        "* Here is one suggested setups: use 6 layers of trnasformer encoder layer, 4 multi heads, dropout = 0.1, and batch_first = True. (and you can definitely find a better one as you would prefer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuJIzF3gicZv"
      },
      "outputs": [],
      "source": [
        "class TextTransformerClassifier(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    vocab_size: int,\n",
        "    num_classes: int=2,\n",
        "    embedding_dim: int=256,\n",
        "    max_seq_length: int=256,\n",
        "    pad_id: int = 0,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # Since the input is only scalar, we use embedding layer to convert a scalar to an embeddings.\n",
        "    # This is similar to the position embedding.\n",
        "    # The input to this layer is [batch_size, sequence_length],\n",
        "    # and the output is [batch_size, sequence_length, embedding_dim].\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
        "\n",
        "    # Define the positional encoding layer.\n",
        "    self.positional_encoding = PositionalEncoding(\n",
        "        embedding_dim, max_seq_length=max_seq_length)\n",
        "\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=4,\n",
        "            dim_feedforward=4 * embedding_dim,\n",
        "            dropout=0.1,\n",
        "            batch_first=True  # Input shape: (batch, seq, features)\n",
        "    )\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "    # Classification head\n",
        "    self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "  def forward(self, input_ids: torch.Tensor, key_padding_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Forward pass of the encoder classifier.\n",
        "\n",
        "    Args:\n",
        "      input_ids: [batch_size, sequence_length],\n",
        "      key_padding_mask: [batch_size, sequence_length] (True mean padded token,\n",
        "          will be ignored by the Transformer encoder)\n",
        "\n",
        "    Returns:\n",
        "      The output tensor with shape [batch_size, num_classes].\n",
        "    \"\"\"\n",
        "    # 1. Convert the input ids to embeddings\n",
        "    # The input to this layer is [batch_size, sequence_length],\n",
        "    # and the output is [batch_size, sequence_length, embedding_dim].\n",
        "    seq_embedded = self.embed(input_ids)\n",
        "\n",
        "    # Do the rest of transformer:\n",
        "    # 2. Add positional information\n",
        "    # 3. Transformer\n",
        "    # 4. MLP head to CLS token\n",
        "    #\n",
        "    # Note:\n",
        "    #   1. Unlike the previous ViT, here we already attach CLS\n",
        "    #      token to the data preparation. So we don't need to\n",
        "    #      concatenate the CLS token here.\n",
        "    #   2. When calling the transformer encoder, src_key_padding_mask\n",
        "    #      is used to mask the padding tokens in the input.\n",
        "    #####################\n",
        "    # Your code goes here\n",
        "    #####################\n",
        "    seq_embedded = self.positional_encoding(seq_embedded)\n",
        "\n",
        "    encoded = self.transformer_encoder(\n",
        "            seq_embedded,\n",
        "            src_key_padding_mask=key_padding_mask\n",
        "    )  # (batch, seq_len, embedding_dim)\n",
        "\n",
        "    # Extract CLS token (first token) and classify\n",
        "    cls_token = encoded[:, 0, :]\n",
        "    output = self.classifier(cls_token)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZpSXIv682en"
      },
      "source": [
        "## Problem D.3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUrnyl8M-sim"
      },
      "source": [
        "Similar, let us use GPU this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z29_kSch-sim",
        "outputId": "5a41724f-be5f-4e34-e3ab-0e50356fc525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ldN_CLOCpV"
      },
      "source": [
        "Write your training and evaluation code.\n",
        "\n",
        "If your model is defined correctly, you should get at least 81% accuracy.\n",
        "\n",
        "The training should be less than 30min on T4.\n",
        "\n",
        "Note:\n",
        "* Your code should print the testing accuracy on the testing set at the very end.\n",
        "\n",
        "Hint:\n",
        "* You can still use nn.CrossEntropyLoss()\n",
        "* vocab_size should be `vocab_cap`\n",
        "* You can refer to the `train_one_epoch` and `test_all_sample` above, but note that the input now has both `ids` and `key_masking`, and you need pass them both to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSiwwuR1c1Kc",
        "outputId": "6c1fb58e-ea4b-4f3d-c0ee-9ed8f44f430c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.714975 [   32/25000] \n",
            "loss: 0.735282 [ 3232/25000] \n",
            "loss: 0.627558 [ 6432/25000] \n",
            "loss: 0.682381 [ 9632/25000] \n",
            "loss: 0.560042 [12832/25000] \n",
            "loss: 0.428585 [16032/25000] \n",
            "loss: 0.489282 [19232/25000] \n",
            "loss: 0.719158 [22432/25000] \n",
            "One epoch takes 98.43339204788208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:408: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.529089 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.423075 [   32/25000] \n",
            "loss: 0.721571 [ 3232/25000] \n",
            "loss: 0.504981 [ 6432/25000] \n",
            "loss: 0.384265 [ 9632/25000] \n",
            "loss: 0.326023 [12832/25000] \n",
            "loss: 0.599656 [16032/25000] \n",
            "loss: 0.297390 [19232/25000] \n",
            "loss: 0.411415 [22432/25000] \n",
            "One epoch takes 98.96257376670837\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.389574 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.301252 [   32/25000] \n",
            "loss: 0.194774 [ 3232/25000] \n",
            "loss: 0.392025 [ 6432/25000] \n",
            "loss: 0.302181 [ 9632/25000] \n",
            "loss: 0.292153 [12832/25000] \n",
            "loss: 0.497486 [16032/25000] \n",
            "loss: 0.271043 [19232/25000] \n",
            "loss: 0.318838 [22432/25000] \n",
            "One epoch takes 98.7974123954773\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.381251 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.292259 [   32/25000] \n",
            "loss: 0.403503 [ 3232/25000] \n",
            "loss: 0.236165 [ 6432/25000] \n",
            "loss: 0.433466 [ 9632/25000] \n",
            "loss: 0.272281 [12832/25000] \n",
            "loss: 0.234620 [16032/25000] \n",
            "loss: 0.422258 [19232/25000] \n",
            "loss: 0.299185 [22432/25000] \n",
            "One epoch takes 99.01807284355164\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.445974 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.306035 [   32/25000] \n",
            "loss: 0.102074 [ 3232/25000] \n",
            "loss: 0.199191 [ 6432/25000] \n",
            "loss: 0.142342 [ 9632/25000] \n",
            "loss: 0.214665 [12832/25000] \n",
            "loss: 0.483215 [16032/25000] \n",
            "loss: 0.359752 [19232/25000] \n",
            "loss: 0.454060 [22432/25000] \n",
            "One epoch takes 99.02821969985962\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.432295 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.432295 \n",
            "\n",
            "Training done!\n"
          ]
        }
      ],
      "source": [
        "#####################\n",
        "# Your code goes here\n",
        "#####################\n",
        "import numpy as np\n",
        "\n",
        "def train_one_epoch(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: str='cpu',\n",
        "    loss_print_iter: int=100\n",
        "  ):\n",
        "  num_train_samples = len(dataloader.dataset)\n",
        "\n",
        "  # Set the model to the training mod\n",
        "  model.train()\n",
        "  all_losses = []\n",
        "  all_acc = []\n",
        "\n",
        "  for batch_index, (ids, key_masking, label) in enumerate(dataloader):\n",
        "    ids = ids.to(device)\n",
        "    key_masking = key_masking.to(device)\n",
        "    label = label.to(device)\n",
        "    pred = model(ids, key_masking)\n",
        "    loss = loss_fn(pred, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss\n",
        "    all_losses.append(loss.item())\n",
        "    acc = ((pred.argmax(1) == label).type(torch.float).sum().item() /\n",
        "           ids.shape[0])\n",
        "    all_acc.append(acc)\n",
        "\n",
        "    if batch_index % loss_print_iter == 0:\n",
        "      loss, trained_samples = loss.item(), (batch_index + 1) * ids.shape[0]\n",
        "      print(f'loss: {loss:>7f} '\n",
        "            f'[{trained_samples:>5d}/{num_train_samples:>5d}] ')\n",
        "\n",
        "  return all_losses, all_acc\n",
        "\n",
        "def test_all_samples(\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    device: str='cpu'\n",
        ") -> None:\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  num_testing_samples = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Disable gradient calculation for inference\n",
        "  with torch.no_grad():\n",
        "    for ids, key_masking, label in dataloader:\n",
        "      ids, key_masking = ids.to(device), key_masking.to(device)\n",
        "      label = label.to(device)\n",
        "      pred = model(ids, key_masking)\n",
        "      test_loss += loss_fn(pred, label).item()\n",
        "      correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  acc = correct / num_testing_samples\n",
        "  print(f'Test Error: \\n Accuracy: {(100*acc):>0.1f}%, '\n",
        "        f'Avg loss: {test_loss:>8f} \\n')\n",
        "  return test_loss, acc\n",
        "\n",
        "\n",
        "model = TextTransformerClassifier(vocab_size=vocab_cap)\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5   # Number of training epochs\n",
        "optimizer = torch.optim.Adam(\n",
        "  model.parameters(),    # All trainable parameters.\n",
        "  lr=1e-4                # Learning rate\n",
        ")\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  start_time =time.time()\n",
        "  _, _ = train_one_epoch(train_loader, model, loss_fn, optimizer, device=device)\n",
        "  print(f'One epoch takes {time.time() - start_time}')\n",
        "  _, _ = test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "test_all_samples(test_loader, model, loss_fn, device=device)\n",
        "\n",
        "print(\"Training done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
